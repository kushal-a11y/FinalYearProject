{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c716d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f88c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "            caseid v000  v001  v002  v003  v004    v005  v006  v007  v008  \\\n",
      "0    0100103022 04  IA7   130    22     4   130  201224    12  2019  1440   \n",
      "1    0100103033 02  IA7   130    33     2   130  201224    12  2019  1440   \n",
      "2    0100100997 02  IA7   109    97     2   109  196628    11  2019  1439   \n",
      "3    0100100966 02  IA7   109    66     2   109  196628    11  2019  1439   \n",
      "4    0100100914 02  IA7   109    14     2   109  196628    11  2019  1439   \n",
      "\n",
      "   ...  smb81  smb305  smb306  sm190s   sm191s  sm190us  sm191us  sm190rs  \\\n",
      "0  ...    0.0    70.1    77.2       5  1264770      NaN      NaN      5.0   \n",
      "1  ...    0.0    92.5    96.3       2   144090      NaN      NaN      3.0   \n",
      "2  ...    0.0    72.8    78.2       2   103120      NaN      NaN      3.0   \n",
      "3  ...    0.0    81.2    84.4       1  -460030      NaN      NaN      2.0   \n",
      "4  ...    0.0    81.4    84.5       3   626660      NaN      NaN      4.0   \n",
      "\n",
      "     sm191rs  smcrefuse  \n",
      "0  1690700.0        NaN  \n",
      "1   459430.0        NaN  \n",
      "2   414410.0        NaN  \n",
      "3  -204300.0        NaN  \n",
      "4   989620.0        NaN  \n",
      "\n",
      "[5 rows x 3120 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your file\n",
    "file_path = r\"D:\\FinalYearProject\\Matrimony_Matchmaker\\data\\DHS\\IACR7EFL.DTA\"\n",
    "\n",
    "# Use the StataReader to get the data and the labels separately\n",
    "with pd.io.stata.StataReader(file_path) as reader:\n",
    "    # 1. Load the data without converting categoricals\n",
    "    df = reader.read(convert_categoricals=False)\n",
    "    \n",
    "    # 2. Extract the value labels (dictionary) if you need them later\n",
    "    # This stores what 1, 2, 3 mean for each column\n",
    "    value_labels = reader.value_labels()\n",
    "\n",
    "# Now the dataframe will load successfully\n",
    "print(\"Data loaded successfully!\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f23dbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([991, 992, 993, 998], dtype=int16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"v131\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99773c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction successful! 57693 couples processed.\n",
      "       ID  Wife_Age      Wife_Education Wife_Profession Wife_Caste  \\\n",
      "0  130_22        29  Secondary or below        Services      Caste   \n",
      "1  130_33        36  Secondary or below     Not Working      Caste   \n",
      "2  109_97        48  Secondary or below     Not Working      Tribe   \n",
      "3  109_66        39  Secondary or below     Not Working      Caste   \n",
      "4  109_14        31  Secondary or below     Not Working      Tribe   \n",
      "\n",
      "  Wife_Religion Wife_Residence  Husband_Age   Husband_Education  \\\n",
      "0        Muslim          Rural           31    Higher Secondary   \n",
      "1        Muslim          Rural           42  Secondary or below   \n",
      "2        Muslim          Rural           54  Secondary or below   \n",
      "3        Muslim          Rural           38  Secondary or below   \n",
      "4        Muslim          Rural           39    Higher Secondary   \n",
      "\n",
      "  Husband_Profession Husband_Caste Husband_Religion Husband_Residence  \\\n",
      "0           Services         Caste           Muslim             Rural   \n",
      "1              Sales         Caste           Muslim             Rural   \n",
      "2       Agricultural         Tribe           Muslim             Rural   \n",
      "3       Agricultural         Caste           Muslim             Rural   \n",
      "4       Agricultural         Tribe           Muslim             Rural   \n",
      "\n",
      "  Conflict_Score  \n",
      "0             No  \n",
      "1             No  \n",
      "2             No  \n",
      "3             No  \n",
      "4             No  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data raw to avoid Stata label conflicts\n",
    "file_path = r\"D:\\FinalYearProject\\Matrimony_Matchmaker\\data\\DHS\\IACR7EFL.DTA\"\n",
    "with pd.io.stata.StataReader(file_path) as reader:\n",
    "    raw_df = reader.read(convert_categoricals=False)\n",
    "\n",
    "# Standardize column names to lowercase to match the DCT file provided\n",
    "raw_df.columns = [col.lower() for col in raw_df.columns]\n",
    "\n",
    "# Helper function to map education years to your specific B.Tech/MBA format\n",
    "def map_degree(years):\n",
    "    if pd.isna(years) or years > 90: return \"N/A\"\n",
    "    if years >= 17: return \"Post-Graduate (MBA/MA)\"\n",
    "    if years >= 15: return \"Graduate (B.Tech/B.Sc/BA)\"\n",
    "    if years >= 12: return \"Higher Secondary\"\n",
    "    return \"Secondary or below\"\n",
    "\n",
    "# Helper function to map profession codes from the labels provided\n",
    "def map_occ(code):\n",
    "    occ_map = {0: \"Not Working\", 1: \"Professional/IT/Tech\", 3: \"Clerical\", 4: \"Sales\", \n",
    "               5: \"Services\", 6: \"Agricultural\", 7: \"Manual\"}\n",
    "    return occ_map.get(code, \"Other\")\n",
    "\n",
    "processed_df = pd.DataFrame()\n",
    "\n",
    "# 1. IDENTIFICATION\n",
    "processed_df['ID'] = raw_df['v001'].astype(str) + \"_\" + raw_df['v002'].astype(str)\n",
    "\n",
    "# 2. FEMALE FEATURES (Respondent)\n",
    "processed_df['Wife_Age'] = raw_df['v012']\n",
    "processed_df['Wife_Education'] = raw_df['v133'].apply(map_degree)\n",
    "processed_df['Wife_Profession'] = raw_df['v717'].apply(map_occ)\n",
    "processed_df['Wife_Caste'] = raw_df['v131'].map({991: 'Caste', 992: 'Tribe', 993: 'No Caste'})\n",
    "processed_df['Wife_Religion'] = raw_df['v130'].map({1: 'Hindu', 2: 'Muslim', 3: 'Christian', 4: 'Sikh', 5: 'Buddhist', 6: 'Jain'})\n",
    "processed_df['Wife_Residence'] = raw_df['v025'].map({1: 'Urban', 2: 'Rural'})\n",
    "\n",
    "# 3. MALE FEATURES (Husband/Partner)\n",
    "processed_df['Husband_Age'] = raw_df['v730'] # v730 is Husband's age in the Couple file\n",
    "processed_df['Husband_Education'] = raw_df['mv133'].apply(map_degree)\n",
    "processed_df['Husband_Profession'] = raw_df['mv717'].apply(map_occ)\n",
    "processed_df['Husband_Caste'] = raw_df['mv131'].map({991: 'Caste', 992: 'Tribe', 993: 'No Caste'})\n",
    "processed_df['Husband_Religion'] = raw_df['mv130'].map({1: 'Hindu', 2: 'Muslim', 3: 'Christian', 4: 'Sikh', 5: 'Buddhist', 6: 'Jain'})\n",
    "processed_df['Husband_Residence'] = raw_df['mv025'].map({1: 'Urban', 2: 'Rural'})\n",
    "\n",
    "# 4. SUCCESS METRIC (Domestic Conflict Proxy)\n",
    "# d105a: \"Ever been pushed, shook or had something thrown by husband\"\n",
    "processed_df['Conflict_Score'] = raw_df['d105a'].replace({0: 'No', 1: 'Yes', 2: 'Yes', 3: 'Yes', 4: 'Yes'})\n",
    "\n",
    "# 5. SAVE\n",
    "output_path = r\"D:\\FinalYearProject\\Matrimony_Matchmaker\\data\\DHS\\final_couples_profile.csv\"\n",
    "processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Extraction successful! {len(processed_df)} couples processed.\")\n",
    "print(processed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32e86a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalized dataframe created with descriptive column names.\n",
      "['ID', 'Wife_Age', 'Wife_Education', 'Wife_Profession', 'Wife_Caste', 'Wife_Religion', 'Wife_Residence', 'Husband_Age', 'Husband_Education', 'Husband_Profession', 'Husband_Caste', 'Husband_Religion', 'Husband_Residence', 'Conflict_Score', 'Emotional_Conflict', 'Physical_Conflict', 'Controlling_Negativity', 'Lack_of_Agency', 'Total_Marital_Conflict_Score', 'Marriage_Successful', 'Ever_been_humiliated_by_husband_partner', 'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner', 'Ever_been_pushed_shook_or_had_something_thrown_by_husband_partner', 'Ever_been_slapped_by_husband_partner', 'Husband_partner_jealous_if_respondent_talks_with_other_men', 'Husband_partner_does_not_permit_respondent_to_meet_female_friends', 'Husband_partner_insists_on_knowing_where_respondent_is', 'Person_who_usually_decides_on_respondents_health_care', 'Person_who_usually_decides_on_large_household_purchases', 'Respondent_afraid_of_husband_partner_most_of_the_time_sometimes_or_never']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the raw dataset\n",
    "# raw_df = pd.read_stata(r\"D:\\FinalYearProject\\Matrimony_Matchmaker\\data\\DHS\\IACR7EFL.DTA\", convert_categoricals=False)\n",
    "# raw_df.columns = [col.lower() for col in raw_df.columns]\n",
    "\n",
    "# --- 1. Map Columns to their full Dictionary Descriptions ---\n",
    "# These descriptions are taken exactly from your provided .txt and .dct files\n",
    "\n",
    "satisfaction_mapping = {\n",
    "    # Emotional/Verbal\n",
    "    'd103a': \"Ever_been_humiliated_by_husband_partner\",\n",
    "    'd103c': \"Ever_been_insulted_or_made_to_feel_bad_by_husband_partner\",\n",
    "    \n",
    "    # Physical Conflict\n",
    "    'd105a': \"Ever_been_pushed_shook_or_had_something_thrown_by_husband_partner\",\n",
    "    'd105b': \"Ever_been_slapped_by_husband_partner\",\n",
    "    \n",
    "    # Controlling Behavior\n",
    "    'd101a': \"Husband_partner_jealous_if_respondent_talks_with_other_men\",\n",
    "    'd101c': \"Husband_partner_does_not_permit_respondent_to_meet_female_friends\",\n",
    "    'd101e': \"Husband_partner_insists_on_knowing_where_respondent_is\",\n",
    "    \n",
    "    # Agency / Decision Making\n",
    "    'v743a': \"Person_who_usually_decides_on_respondents_health_care\",\n",
    "    'v743b': \"Person_who_usually_decides_on_large_household_purchases\",\n",
    "    \n",
    "    # Fear\n",
    "    'd129': \"Respondent_afraid_of_husband_partner_most_of_the_time_sometimes_or_never\"\n",
    "}\n",
    "\n",
    "# --- 2. Add these to your finalized dataframe ---\n",
    "\n",
    "# We iterate through the mapping and add the raw values under the descriptive headers\n",
    "for code, description in satisfaction_mapping.items():\n",
    "    if code in raw_df.columns:\n",
    "        processed_df[description] = raw_df[code]\n",
    "\n",
    "# --- 3. Optional: Convert numeric codes to simple \"Yes/No\" or \"Problem\" text \n",
    "# for better readability in your final CSV\n",
    "\n",
    "# For the Conflict/Violence variables (d series), 0 is 'No', >0 is 'Yes'\n",
    "conflict_cols = [\n",
    "    \"Ever_been_humiliated_by_husband_partner\",\n",
    "    \"Ever_been_insulted_or_made_to_feel_bad_by_husband_partner\",\n",
    "    \"Ever_been_pushed_shook_or_had_something_thrown_by_husband_partner\",\n",
    "    \"Ever_been_slapped_by_husband_partner\",\n",
    "    \"Husband_partner_jealous_if_respondent_talks_with_other_men\",\n",
    "    \"Husband_partner_does_not_permit_respondent_to_meet_female_friends\",\n",
    "    \"Husband_partner_insists_on_knowing_where_respondent_is\"\n",
    "]\n",
    "\n",
    "for col in conflict_cols:\n",
    "    if col in processed_df.columns:\n",
    "        processed_df[col] = np.where(processed_df[col] > 0, \"Yes/Problem\", \"No/Never\")\n",
    "\n",
    "# For Decision Making (v743 series), keep numeric or map based on your labels:\n",
    "# 1=Wife, 2=Joint, 4=Husband, 5=Other\n",
    "# No categorization requested, so keeping as is.\n",
    "\n",
    "# --- 4. Save the finalized file ---\n",
    "output_path = r\"D:\\FinalYearProject\\Matrimony_Matchmaker\\data\\DHS\\finalized_descriptive_marriage_data.csv\"\n",
    "processed_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Finalized dataframe created with descriptive column names.\")\n",
    "print(processed_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "736a86e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Secondary or below', 'Higher Secondary',\n",
       "       'Graduate (B.Tech/B.Sc/BA)', 'Post-Graduate (MBA/MA)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"Wife_Education\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02e3dda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Services', 'Sales', 'Agricultural', 'Professional/IT/Tech',\n",
       "       'Manual', 'Other', 'Clerical', 'Not Working'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"Husband_Profession\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2727790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Muslim', 'Hindu', 'Sikh', nan, 'Buddhist', 'Jain', 'Christian'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"Wife_Religion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a4faa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Muslim', 'Hindu', 'Sikh', 'Christian', nan, 'Buddhist', 'Jain'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"Husband_Religion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87348cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rural', 'Urban'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[\"Husband_Residence\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0915694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11396\\1347343322.py:58: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers and Answers decoded for all 3000+ columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the raw dataset\n",
    "file_path = r\"D:\\FinalYearProject\\Matrimony_Matchmaker\\data\\DHS\\IACR7EFL.DTA\"\n",
    "with pd.io.stata.StataReader(file_path) as reader:\n",
    "    raw_df = reader.read(convert_categoricals=False)\n",
    "\n",
    "raw_df.columns = [col.lower() for col in raw_df.columns]\n",
    "# 1. Load raw data (lowercase columns to match text file)\n",
    "# raw_df = pd.read_stata('IACR7EFL.DTA', convert_categoricals=False)\n",
    "raw_df.columns = [col.lower() for col in raw_df.columns]\n",
    "\n",
    "# 2. Read the dictionary text\n",
    "with open('Couple_data_meaning of coulmns.txt', 'r') as f:\n",
    "    dict_content = f.read()\n",
    "\n",
    "# --- PART A: MAP COLUMN HEADERS ---\n",
    "name_map = {}\n",
    "header_pattern = re.compile(r'label variable\\s+(\\w+)\\s+\"(.+?)\"', re.IGNORECASE)\n",
    "for match in header_pattern.finditer(dict_content):\n",
    "    clean_desc = match.group(2).replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    name_map[match.group(1).lower()] = clean_desc\n",
    "\n",
    "# --- PART B: MAP VALUE MEANINGS (THE ANSWERS) ---\n",
    "\n",
    "# 1. Extract the Answer Keys (label define blocks)\n",
    "# This captures: label define V025 1 \"Urban\" 2 \"Rural\"\n",
    "value_labels = {}\n",
    "define_pattern = re.compile(r'label define\\s+(\\w+)\\s*(.*?);', re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "for match in define_pattern.finditer(dict_content):\n",
    "    label_name = match.group(1).lower()\n",
    "    mapping_text = match.group(2)\n",
    "    \n",
    "    # Parse individual pairs like: 1 \"Urban\"\n",
    "    pairs = re.findall(r'(\\d+)\\s+\"(.+?)\"', mapping_text)\n",
    "    value_labels[label_name] = {int(num): txt for num, txt in pairs}\n",
    "\n",
    "# 2. Link Columns to Answer Keys (label values lines)\n",
    "# This captures: label values v025 V025 (Linking column v025 to the Urban/Rural key)\n",
    "column_to_label_link = {}\n",
    "link_pattern = re.compile(r'label values\\s+(\\w+)\\s+(\\w+)', re.IGNORECASE)\n",
    "\n",
    "for match in link_pattern.finditer(dict_content):\n",
    "    col_name = match.group(1).lower()\n",
    "    label_key = match.group(2).lower()\n",
    "    column_to_label_link[col_name] = label_key\n",
    "\n",
    "# 3. Apply the mappings to the dataframe\n",
    "decoded_df = raw_df.copy()\n",
    "\n",
    "for col in decoded_df.columns:\n",
    "    if col in column_to_label_link:\n",
    "        label_key = column_to_label_link[col]\n",
    "        if label_key in value_labels:\n",
    "            # Replace numbers with text using the extracted dictionary\n",
    "            decoded_df[col] = decoded_df[col].map(value_labels[label_key]).fillna(decoded_df[col])\n",
    "\n",
    "# --- PART C: ORDERING AND SAVING ---\n",
    "h_list = ['v730', 'mv438', 'mv133', 'mv717', 'mv131', 'mv130']\n",
    "w_list = ['v012', 'v438', 'v133', 'v717', 'v131', 'v130', 'v025', 'v024']\n",
    "c_list = ['d101a', 'd101e', 'd103a', 'd103c', 'd105a', 'd105b', 'v743a', 'v743b', 'd129']\n",
    "\n",
    "priority_cols = [c for c in (h_list + w_list + c_list) if c in decoded_df.columns]\n",
    "other_cols = [c for c in decoded_df.columns if c not in priority_cols]\n",
    "\n",
    "final_df = decoded_df[priority_cols + other_cols].copy()\n",
    "final_df.rename(columns=name_map, inplace=True)\n",
    "\n",
    "# Save\n",
    "final_df.to_csv('Fully_Decoded_India_Dataset.csv', index=False)\n",
    "print(\"Headers and Answers decoded for all 3000+ columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedcf319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f9233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_4888\\3686749774.py:1: DtypeWarning: Columns (6,93,94,95,96,109,110,128,132,157,191,199,260,261,262,263,264,265,266,267,268,269,270,271,340,341,342,343,344,345,346,347,348,349,350,351,360,361,362,363,364,365,366,367,368,369,370,371,376,377,378,379,380,381,382,383,384,385,386,389,440,441,442,443,444,445,446,447,448,449,450,460,461,462,463,464,465,466,467,468,469,470,471,476,478,479,480,497,499,500,501,517,518,519,520,521,522,523,524,525,526,527,528,529,531,539,540,541,542,543,544,545,546,547,548,549,550,558,559,560,561,562,563,564,565,566,567,568,569,570,675,676,694,695,696,713,717,785,791,798,873,880,895,990,1003,1016,1019,1047,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1148,1191,1192,1193,1239,1274,1278,1287,1298,1308,1309,1318,1319,1320,1336,1337,1340,1341,1342,1356,1362,1423,1487,1564,1565,1566,1567,1625,1626,1628,1629,1699,1700,1703,1710,1716,1719,1721,1727,1737,1760,1761,1764,1781,1785,1786,1787,1788,1789,1790,1791,1796,1816,1817,1818,1820,1821,1851,1852,1879,1880,1881,1882,1885,1886,1887,1888,1897,1898,1899,1900,1909,1910,1911,1912,1927,1928,1929,1930,1933,1934,1935,1936,1939,1940,1941,1942,1957,1958,1959,1960,1963,1964,1965,1966,1969,1970,1971,1972,1975,1976,1977,1978,1993,1994,1995,1996,1999,2000,2001,2002,2005,2006,2007,2008,2035,2036,2053,2054,2055,2089,2090,2161,2162,2179,2180,2181,2197,2198,2199,2215,2216,2275,2286,2288,2325,2404,2406,2407,2409,2410,2411,2422,2423,2424,2425,2426,2427,2428,2431,2432,2433,2434,2435,2436,2437,2438,2439,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2462,2463,2464,2465,2466,2468,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2530,2532,2533,2534,2535,2536,2555,2556,2557,2558,2564,2565,2566,2567,2591,2592,2609,2611,2612,2616,2617,2618,2619,2620,2623,2630,2632,2636,2638,2640,2642,2644,2672,2673,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2794,2800,2803,2806,2810,2811,2821,2832,2835,2836,2837,2838,2843,2862,2863,2864,2865,2866,2867,2868,2869,2915,2916,2926,2927,2937,2938,2939,2940,2941,2942,2943,2944,2945,2970,2972,2973,2976,2982,2989,2993,2995,2997,2999,3001,3003,3092,3098,3101,3104,3108,3109,3117) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"Fully_Decoded_India_Dataset.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Fully_Decoded_India_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b859b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame shape: (57693, 22)\n",
      "   Husband_age  Husband years_of_education       Husband Occupation_(grouped)  \\\n",
      "0           31                          12  Services / household and domestic   \n",
      "1           42                           5                              Sales   \n",
      "2           54                           8                       Agricultural   \n",
      "3           38                           7                       Agricultural   \n",
      "4           39                          12                       Agricultural   \n",
      "\n",
      "  Husband Ethnicity Husband Religion  Wife's_current_age  \\\n",
      "0             Caste           Muslim                  29   \n",
      "1             Caste           Muslim                  36   \n",
      "2             Tribe           Muslim                  48   \n",
      "3             Caste           Muslim                  39   \n",
      "4             Tribe           Muslim                  31   \n",
      "\n",
      "  Wife's_height_in_centimeters_(1_decimal)  Wife's Education_in_years  \\\n",
      "0                                     1617                          8   \n",
      "1                                     1504                          3   \n",
      "2                                     1582                          0   \n",
      "3                                     1568                         10   \n",
      "4                                     1548                         10   \n",
      "\n",
      "         Wife's_occupation_(grouped) Wife's Ethnicity  ...            State  \\\n",
      "0  Services / household and domestic            Caste  ...  Jammu & Kashmir   \n",
      "1                        Not working            Caste  ...  Jammu & Kashmir   \n",
      "2                        Not working            Tribe  ...  Jammu & Kashmir   \n",
      "3                        Not working            Caste  ...  Jammu & Kashmir   \n",
      "4                        Not working            Tribe  ...  Jammu & Kashmir   \n",
      "\n",
      "  Husband_partner_jealous_if_Wife_talks_with_other_men  \\\n",
      "0                                                 No     \n",
      "1                                                Yes     \n",
      "2                                                 No     \n",
      "3                                                 No     \n",
      "4                                                 No     \n",
      "\n",
      "  Husband_partner_insists_on_knowing_where_Wife_is  \\\n",
      "0                                               No   \n",
      "1                                               No   \n",
      "2                                               No   \n",
      "3                                               No   \n",
      "4                                               No   \n",
      "\n",
      "  Ever_been_humiliated_by_husband_partner  \\\n",
      "0                                   Never   \n",
      "1                                   Never   \n",
      "2                                   Never   \n",
      "3                                   Never   \n",
      "4                                   Never   \n",
      "\n",
      "  Ever_been_insulted_or_made_to_feel_bad_by_husband_partner  \\\n",
      "0                                              Never          \n",
      "1                                              Never          \n",
      "2                                              Never          \n",
      "3                                              Never          \n",
      "4                                              Never          \n",
      "\n",
      "  Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner  \\\n",
      "0                                              Never                   \n",
      "1                                              Never                   \n",
      "2                                              Never                   \n",
      "3                                              Never                   \n",
      "4                                              Never                   \n",
      "\n",
      "  Ever_been_slapped_by_husband_partner  \\\n",
      "0                                Never   \n",
      "1                                Never   \n",
      "2                                Never   \n",
      "3                                Never   \n",
      "4                                Never   \n",
      "\n",
      "  Person_who_usually_decides_on_Wife's_health_care  \\\n",
      "0                   Respondent and husband/partner   \n",
      "1                   Respondent and husband/partner   \n",
      "2                   Respondent and husband/partner   \n",
      "3                   Respondent and husband/partner   \n",
      "4                            Husband/partner alone   \n",
      "\n",
      "  Person_who_usually_decides_on_large_household_purchases  \\\n",
      "0                     Respondent and husband/partner        \n",
      "1                     Respondent and husband/partner        \n",
      "2                     Respondent and husband/partner        \n",
      "3                     Respondent and husband/partner        \n",
      "4                              Husband/partner alone        \n",
      "\n",
      "  Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never  \n",
      "0                                   Sometimes afraid                   \n",
      "1                                   Sometimes afraid                   \n",
      "2                                       Never afraid                   \n",
      "3                                   Sometimes afraid                   \n",
      "4                                       Never afraid                   \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the exact column names you want to keep\n",
    "features = [\n",
    "    'Husband_age', \n",
    "    'Husband years_of_education', \n",
    "    'Husband Occupation_(grouped)', \n",
    "    'Husband Ethnicity', \n",
    "    'Husband Religion', \n",
    "    \"Wife's_current_age\", \n",
    "    \"Wife's_height_in_centimeters_(1_decimal)\", \n",
    "    \"Wife's Education_in_years\", \n",
    "    \"Wife's_occupation_(grouped)\", \n",
    "    \"Wife's Ethnicity\", \n",
    "    \"Wife's Religion\", \n",
    "    'residence', \n",
    "    'State'\n",
    "]\n",
    "\n",
    "targets = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men', \n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is', \n",
    "    'Ever_been_humiliated_by_husband_partner', \n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner', \n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner', \n",
    "    'Ever_been_slapped_by_husband_partner', \n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\", \n",
    "    \"Person_who_usually_decides_on_large_household_purchases\", \n",
    "    \"Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never\"\n",
    "]\n",
    "\n",
    "# Create the new DataFrame with only these columns\n",
    "# We use intersection() to avoid errors if a column name has a slight typo\n",
    "cols_to_keep = [c for c in (features + targets) if c in df.columns]\n",
    "final_df = df[cols_to_keep].copy()\n",
    "\n",
    "# View the result\n",
    "print(f\"New DataFrame shape: {final_df.shape}\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to map education years to your specific B.Tech/MBA format\n",
    "def map_degree(years):\n",
    "    if pd.isna(years) or years > 90: return \"N/A\"\n",
    "    if years >= 17: return \"Post-Graduate (MBA/MA)\"\n",
    "    if years >= 15: return \"Graduate (B.Tech/B.Sc/BA)\"\n",
    "    if years >= 12: return \"Higher Secondary\"\n",
    "    return \"Secondary or below\"\n",
    "final_df[\"Husband years_of_education\"] = final_df[\"Husband years_of_education\"].apply(map_degree)\n",
    "final_df[\"Wife's Education_in_years\"] = final_df[\"Wife's Education_in_years\"].apply(map_degree)\n",
    "final_df.rename(columns={\"Husband years_of_education\" : \"Husband's Education Level\"}, inplace=True)\n",
    "final_df.rename(columns={\"Wife's Education_in_years\" : \"Wife's Education Level\"}, inplace=True)\n",
    "final_df.rename(columns={\"Wife's_height_in_centimeters_(1_decimal)\" : \"Wife's height(centimeters)\"}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f5ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"Wife's height(centimeters)\"] = (\n",
    "    pd.to_numeric(final_df[\"Wife's height(centimeters)\"], errors=\"coerce\") / 10\n",
    ").round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d6cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()\n",
    "final_df.to_csv(\"dataset_final_form.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c80e8",
   "metadata": {},
   "source": [
    "All Survey Questions Values\n",
    "\n",
    "A. Person who usually decides on respondent's (Wife's) health care.\n",
    "    1: Respondent alone (Wife decides)\n",
    "\n",
    "    2: Husband/partner alone (Husband decides)\n",
    "\n",
    "    3: Respondent and husband/partner jointly (Joint Decision - Ideal)\n",
    "\n",
    "    4: Someone else\n",
    "\n",
    "    5: Respondent and someone else\n",
    "\n",
    "    6: Decision not made/not applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b64b1",
   "metadata": {},
   "source": [
    "B.Person who usually decides on large household purchases\n",
    "    1: Respondent alone\n",
    "\n",
    "    2: Husband/partner alone\n",
    "\n",
    "    3: Respondent and husband/partner jointly (Joint Decision - Ideal)\n",
    "\n",
    "    4: Someone else\n",
    "\n",
    "    5: Respondent and someone else\n",
    "\n",
    "    6: Decision not made/not applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc77b2c",
   "metadata": {},
   "source": [
    "C.Husband/partner jealous if wife talks with other men.\n",
    "    0: No\n",
    "\n",
    "    1: Yes\n",
    "    \n",
    "    8: Don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6d742",
   "metadata": {},
   "source": [
    "D.Husband/partner insists on knowing where wife is at all times.\n",
    "\n",
    "    0: No\n",
    "    \n",
    "    1: Yes\n",
    "    \n",
    "    8: Don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce854c9",
   "metadata": {},
   "source": [
    "E.Ever been humiliated by husband/partner.\n",
    "\n",
    "    0: Never\n",
    "\n",
    "    1: Often\n",
    "\n",
    "    2: Sometimes\n",
    "\n",
    "    3: Yes, but not in the last 12 months\n",
    "\n",
    "    4: Yes, but frequency missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41802ee",
   "metadata": {},
   "source": [
    "F.Ever been insulted or made to feel bad by husband/partner.\n",
    "\n",
    "    0: Never\n",
    "\n",
    "    1: Often\n",
    "\n",
    "    2: Sometimes\n",
    "\n",
    "    3: Yes, but not in the last 12 months\n",
    "\n",
    "    4: Yes, but frequency missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4b5a5",
   "metadata": {},
   "source": [
    "G.Ever been pushed, shook, or had something thrown by husband/partner.\n",
    "\n",
    "    0: Never\n",
    "    \n",
    "    1: Often\n",
    "    \n",
    "    2: Sometimes\n",
    "    \n",
    "    3: Yes, but not in the last 12 months\n",
    "    \n",
    "    4: Yes, but frequency missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633d3a7",
   "metadata": {},
   "source": [
    "H.Ever been slapped by husband/partner.\n",
    "\n",
    "    0: Never\n",
    "\n",
    "    1: Often\n",
    "\n",
    "    2: Sometimes\n",
    "\n",
    "    3: Yes, but not in the last 12 months\n",
    "\n",
    "    4: Yes, but frequency missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560d694",
   "metadata": {},
   "source": [
    "I.Wife afraid of husband/partner most of the time, sometimes, or never.\n",
    "\n",
    "    0: Never\n",
    "    \n",
    "    1: Sometimes\n",
    "    \n",
    "    2: Most of the time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335d2d1",
   "metadata": {},
   "source": [
    "Based on the \"Predicting Marital Stability\" (Frank, 2024) paper, \n",
    "the \"Relationship Compatibility\" dimension was built on \n",
    "6 specific psychological questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ae2b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Husband_age', 'Husband's Education Level',\n",
      "       'Husband Occupation_(grouped)', 'Husband Ethnicity', 'Husband Religion',\n",
      "       'Wife's_current_age', 'Wife's height(centimeters)',\n",
      "       'Wife's Education Level', 'Wife's_occupation_(grouped)',\n",
      "       'Wife's Ethnicity', 'Wife's Religion', 'residence', 'State',\n",
      "       'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
      "       'Husband_partner_insists_on_knowing_where_Wife_is',\n",
      "       'Ever_been_humiliated_by_husband_partner',\n",
      "       'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
      "       'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
      "       'Ever_been_slapped_by_husband_partner',\n",
      "       'Person_who_usually_decides_on_Wife's_health_care',\n",
      "       'Person_who_usually_decides_on_large_household_purchases',\n",
      "       'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"/workspaces/FinalYearProject/Matrimony_Matchmaker/data/DHS_DATA.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf94b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows: 57693\n",
      "Complete Responses (To Keep): 46488\n",
      "Incomplete Responses (To Separate): 11205\n",
      "\n",
      "Files saved successfully: 'completeresponse_dhs.csv' and 'incompleteresponse_dhs.csv'\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your dataset (Replace 'your_file.csv' with your actual filename)\n",
    "# df = pd.read_csv('your_file.csv') \n",
    "\n",
    "# 2. Define the 9 Target Columns (Exact names from your list)\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "# 3. Check for \"Missing\" Values\n",
    "# Note: In some datasets, \"missing\" might be represented as NaN, or blank strings, \n",
    "# or specific codes like \"Missing\", \"Don't know\".\n",
    "# We assume standard NaNs here. If you have \"Don't know\" as a string, add it to the replace list.\n",
    "\n",
    "# Optional: Convert \"Don't know\" or \"Missing\" strings to NaN for accurate filtering\n",
    "# df[target_cols] = df[target_cols].replace(['Missing', 'Don\\'t know', '.'], np.nan)\n",
    "\n",
    "# 4. Create the Masks\n",
    "# Mask for Complete Rows: All 9 columns must have a value (not NaN)\n",
    "mask_complete = df[target_cols].notna().all(axis=1)\n",
    "\n",
    "# Mask for Incomplete Rows: At least one column is NaN\n",
    "mask_incomplete = ~mask_complete\n",
    "\n",
    "# 5. Split the Data\n",
    "df_complete = df[mask_complete].copy()\n",
    "df_incomplete = df[mask_incomplete].copy()\n",
    "\n",
    "# 6. Verify Counts\n",
    "print(f\"Total Rows: {len(df)}\")\n",
    "print(f\"Complete Responses (To Keep): {len(df_complete)}\")\n",
    "print(f\"Incomplete Responses (To Separate): {len(df_incomplete)}\")\n",
    "\n",
    "# 7. Save to Files\n",
    "df_complete.to_csv('completeresponse_dhs.csv', index=False)\n",
    "df_incomplete.to_csv('incompleteresponse_dhs.csv', index=False)\n",
    "\n",
    "print(\"\\nFiles saved successfully: 'completeresponse_dhs.csv' and 'incompleteresponse_dhs.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21bb86",
   "metadata": {},
   "source": [
    "The exact Scoring Dictionary to map text-based values to the 04 Risk Scale used in \"Predicting Marital Stability: An Approach for More Characteristics (Frank, 2024)\".\n",
    "\n",
    "The Logic:\n",
    "\n",
    "    0: Perfect Compatibility (Safe, Joint Decisions, No Fear).\n",
    "\n",
    "    4: High Incompatibility (Violence, Total Control, High Fear)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15849913",
   "metadata": {},
   "source": [
    "1. ROLES (Decision Making)\n",
    "-Person_who_usually_decides_on_Wife's_health_careConcept: Egalitarian vs. Hierarchical roles.\n",
    "Text Value to Score:\n",
    "    \"Respondent and husband/partner jointly\" - > 0 (Ideal/Compatible)\n",
    "    \"Respondent alone\" -> 1 (Autonomous/Safe)\n",
    "    \"Husband/partner alone\" -> 4 (Hierarchical/Risk)\n",
    "    \"Someone else\" / \"Other\" -> 4 (Lack of Couple Autonomy)\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9784bee2",
   "metadata": {},
   "source": [
    "2. GOALS (Financial Shared Vision) - Person_who_usually_decides_on_large_household_purchases\n",
    "Text Value to Score: \n",
    "    \"Respondent and husband/partner jointly\" -> 0 (Shared Goals)\n",
    "    \"Respondent alone\" -> 1 (Safe)\n",
    "    \"Husband/partner alone\" -> 4 (Mismatched Goals/Control)\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97381fb6",
   "metadata": {},
   "source": [
    "3. TRUST (Jealousy & Control) - \n",
    "Husband_partner_jealous_if_Wife_talks_with_other_men + \n",
    "Husband_partner_insists_on_knowing_where_Wife_is\n",
    "Text Value to Score:\n",
    "    \"No\" ->0 (High Trust)\n",
    "    \"Yes\" -> 4 (Low Trust / High Risk)\n",
    "    \"Don't know\" -> 2 (Uncertainty)\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e6fd9",
   "metadata": {},
   "source": [
    "4. VIEWS ON MARRIAGE (Boundaries/Violence) -\n",
    "Ever_been_pushed,_shook_or_had_something_thrown + \n",
    "Ever_been_slapped_by_husband_partner\n",
    "    \"Never\" -> 0 (Compatible Views)\n",
    "    \"Yes, but not in the last 12 months\" -> 2 (Past Issues/Moderate Risk)\n",
    "    \"Sometimes\" -> 3 (Ongoing Incompatibility)\n",
    "    \"Often\" -> 4 (Severe Incompatibility)\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce6612",
   "metadata": {},
   "source": [
    "5. LIFE GOALS (Fear)\n",
    "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never\n",
    "    \"Never\" -> 0 (Aligned)\n",
    "    \"Sometimes\" -> 2 (Friction)\n",
    "    \"Most of the time\" -> 4 (Divergent/Toxic)\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ab655",
   "metadata": {},
   "source": [
    "6. DREAMS (Emotional Support vs. Abuse)\n",
    "Ever_been_humiliated_by_husband_partner + \n",
    "Ever_been_insulted_or_made_to_feel_bad...\n",
    "    \"Never\" -> 0 (Supportive)\n",
    "    \"Yes, but not in the last 12 months\" -> 2 (Past Risk)\"Sometimes\" -> 3 (Damaging)\n",
    "    \"Often\" -> 4 (Dream-Killing/High Risk)\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e442ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Frank_Compatibility_Score  Is_Compatible\n",
      "0                   0.666667              1\n",
      "1                   1.000000              1\n",
      "2                   0.333333              1\n",
      "3                   0.666667              1\n",
      "4                   1.333333              0\n",
      "5                   0.333333              1\n",
      "6                   0.333333              1\n",
      "7                   0.333333              1\n",
      "8                   0.333333              1\n",
      "9                   0.333333              1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('completeresponse_dhs.csv')\n",
    "# Load your Complete Data\n",
    "# df = pd.read_csv('completeresponse_dhs.csv')\n",
    "\n",
    "# --- 1. DEFINE SCALING FUNCTIONS ---\n",
    "\n",
    "def score_decisions(val):\n",
    "    s = str(val).lower()\n",
    "    if 'joint' in s: return 0        # Best\n",
    "    if 'respondent' in s: return 1   # Good\n",
    "    if 'husband' in s: return 4      # Bad\n",
    "    if 'someone else' in s: return 4 # Bad\n",
    "    return 2 # Neutral/Unknown\n",
    "\n",
    "def score_binary_risk(val):\n",
    "    # For Jealousy columns\n",
    "    s = str(val).lower()\n",
    "    if 'no' in s: return 0\n",
    "    if 'yes' in s: return 4\n",
    "    return 2\n",
    "\n",
    "def score_frequency_risk(val):\n",
    "    # For Violence/Humiliation columns\n",
    "    s = str(val).lower()\n",
    "    if 'never' in s: return 0\n",
    "    if 'not in the last' in s: return 2\n",
    "    if 'sometimes' in s: return 3\n",
    "    if 'often' in s: return 4\n",
    "    return 2\n",
    "\n",
    "def score_fear(val):\n",
    "    s = str(val).lower()\n",
    "    if 'never' in s: return 0\n",
    "    if 'sometimes' in s: return 2\n",
    "    if 'most' in s: return 4\n",
    "    return 0\n",
    "\n",
    "# --- 2. APPLY MAPPING TO CREATE THE 6 DIMENSIONS ---\n",
    "\n",
    "# Dim 1: Roles\n",
    "df['Score_Roles'] = df[\"Person_who_usually_decides_on_Wife's_health_care\"].apply(score_decisions)\n",
    "\n",
    "# Dim 2: Goals\n",
    "df['Score_Goals'] = df[\"Person_who_usually_decides_on_large_household_purchases\"].apply(score_decisions)\n",
    "\n",
    "# Dim 3: Trust (Average of Jealousy + Control)\n",
    "t1 = df['Husband_partner_jealous_if_Wife_talks_with_other_men'].apply(score_binary_risk)\n",
    "t2 = df['Husband_partner_insists_on_knowing_where_Wife_is'].apply(score_binary_risk)\n",
    "df['Score_Trust'] = (t1 + t2) / 2\n",
    "\n",
    "# Dim 4: Views (Average of Push + Slap)\n",
    "v1 = df['Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner'].apply(score_frequency_risk)\n",
    "v2 = df['Ever_been_slapped_by_husband_partner'].apply(score_frequency_risk)\n",
    "df['Score_Views'] = (v1 + v2) / 2\n",
    "\n",
    "# Dim 5: Life Goals (Fear)\n",
    "df['Score_LifeGoals'] = df[\"Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never\"].apply(score_fear)\n",
    "\n",
    "# Dim 6: Dreams (Average of Humiliation + Insult)\n",
    "d1 = df['Ever_been_humiliated_by_husband_partner'].apply(score_frequency_risk)\n",
    "d2 = df['Ever_been_insulted_or_made_to_feel_bad_by_husband_partner'].apply(score_frequency_risk)\n",
    "df['Score_Dreams'] = (d1 + d2) / 2\n",
    "\n",
    "# --- 3. FINAL AGGREGATION ---\n",
    "# Average of the 6 dimensions\n",
    "df['Frank_Compatibility_Score'] = df[['Score_Roles', 'Score_Goals', 'Score_Trust', \n",
    "                                      'Score_Views', 'Score_LifeGoals', 'Score_Dreams']].mean(axis=1)\n",
    "\n",
    "# --- 4. DETERMINE TARGET CLASS ---\n",
    "# Threshold = 1.227 (from Paper)\n",
    "# <= 1.227 : Compatible (Class 1)\n",
    "# > 1.227  : Incompatible (Class 0)\n",
    "df['Is_Compatible'] = np.where(df['Frank_Compatibility_Score'] <= 1.227, 1, 0)\n",
    "\n",
    "# Save\n",
    "df.to_csv('Target_Variable_Created.csv', index=False)\n",
    "print(df[['Frank_Compatibility_Score', 'Is_Compatible']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930feda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is_Compatible\n",
       "1    31108\n",
       "0    15380\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Is_Compatible'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Aggressive Weight: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/xgboost/training.py:199: UserWarning: [20:52:37] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance at Threshold 0.65 ---\n",
      "Confusion Matrix:\n",
      "[[   2 3074]\n",
      " [   2 6220]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.00      0.00      3076\n",
      "           1       0.67      1.00      0.80      6222\n",
      "\n",
      "    accuracy                           0.67      9298\n",
      "   macro avg       0.58      0.50      0.40      9298\n",
      "weighted avg       0.61      0.67      0.54      9298\n",
      "\n",
      "\n",
      "--- Searching for Best Threshold ---\n",
      "Optimal Threshold for Class 0 Detection: 0.90\n",
      "Best F1-Score for Class 0: 0.015\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_curve\n",
    "\n",
    "# --- 1. LOAD & PREPARE DATA ---\n",
    "df_features = pd.read_csv('completeresponse_dhs.csv')\n",
    "df_target = pd.read_csv('Target_Variable_Created.csv')\n",
    "df = pd.concat([df_features, df_target[['Is_Compatible']]], axis=1)\n",
    "\n",
    "feature_cols = [\n",
    "    'Husband_age', \"Husband's Education Level\", 'Husband Occupation_(grouped)', \n",
    "    'Husband Religion', 'Husband Ethnicity',\n",
    "    \"Wife's_current_age\", \"Wife's Education Level\", \"Wife's_occupation_(grouped)\", \n",
    "    \"Wife's Religion\", \"Wife's Ethnicity\"\n",
    "]\n",
    "\n",
    "X = pd.get_dummies(df[feature_cols], drop_first=True)\n",
    "y = df['Is_Compatible']\n",
    "\n",
    "# --- 2. TRAIN/TEST SPLIT ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- 3. AGGRESSIVE MODEL TRAINING ---\n",
    "# We deliberately OVER-weight the minority class to force the model to learn it.\n",
    "# Standard ratio was ~2. We are pumping it to 10.\n",
    "aggressive_weight = 10 \n",
    "\n",
    "print(f\"\\nTraining with Aggressive Weight: {aggressive_weight}\")\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.03,    # Slower learning to find subtle patterns\n",
    "    max_depth=6,           \n",
    "    scale_pos_weight=aggressive_weight, # FORCE FOCUS ON CLASS 0\n",
    "    subsample=0.8,         # Prevent overfitting\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 4. THRESHOLD TUNING (The Magic Step) ---\n",
    "# Instead of predict(), we get probabilities (0.0 to 1.0)\n",
    "y_proba = model.predict_proba(X_test)[:, 1] # Probability of being Compatible (Class 1)\n",
    "\n",
    "# We want to catch Class 0 (Incompatible).\n",
    "# Standard logic: If Prob(Compatible) < 0.5, then Incompatible.\n",
    "# NEW logic: We will be stricter. We only call it \"Compatible\" if Prob > 0.7.\n",
    "# This makes it much easier to fall into the \"Incompatible\" bucket.\n",
    "\n",
    "decision_threshold = 0.65  # <--- ADJUST THIS TO BALANCE RESULTS\n",
    "y_pred_tuned = (y_proba > decision_threshold).astype(int)\n",
    "\n",
    "# --- 5. EVALUATE ---\n",
    "print(f\"\\n--- Performance at Threshold {decision_threshold} ---\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tuned))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# --- 6. AUTOMATIC OPTIMAL THRESHOLD FINDER ---\n",
    "# This loop finds the threshold that gives the best 'Macro F1' score\n",
    "print(\"\\n--- Searching for Best Threshold ---\")\n",
    "best_thresh = 0\n",
    "best_score = 0\n",
    "for thresh in np.arange(0.3, 0.9, 0.05):\n",
    "    preds = (y_proba > thresh).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    # Focus on Class 0 Recall (Catching Violence)\n",
    "    recall_0 = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    # But don't destroy Precision\n",
    "    precision_0 = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    f1_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0) if (precision_0 + recall_0) > 0 else 0\n",
    "    \n",
    "    if f1_0 > best_score:\n",
    "        best_score = f1_0\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"Optimal Threshold for Class 0 Detection: {best_thresh:.2f}\")\n",
    "print(f\"Best F1-Score for Class 0: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3665337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_features = pd.read_csv('completeresponse_dhs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96596cc8",
   "metadata": {},
   "source": [
    "XGBoost Model is used to predict the ans to all 9 Questions. Since, mapping 9 to 1 marital satisfaction gave poor accuracy and recall.\n",
    "MultiOutPut variant of XGBoost is used.(All 9 questions are treated independently and no correlation among them is consdered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6883bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multi-output XGBoost model...\n",
      "\n",
      "--- Test Accuracy per Survey Question ---\n",
      "Husband_partner_jealous_if_Wife_talks_with_other_men: 74.35%\n",
      "Husband_partner_insists_on_knowing_where_Wife_is: 80.60%\n",
      "Ever_been_humiliated_by_husband_partner: 91.85%\n",
      "Ever_been_insulted_or_made_to_feel_bad_by_husband_partner: 92.77%\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner: 89.17%\n",
      "Ever_been_slapped_by_husband_partner: 76.44%\n",
      "Person_who_usually_decides_on_Wife's_health_care: 73.60%\n",
      "Person_who_usually_decides_on_large_household_purchases: 73.82%\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never: 65.82%\n",
      "\n",
      "Average Overall Test Accuracy: 79.82%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "feature = [c for c in df_features.columns if c not in target_cols]\n",
    "X = df_features[feature].copy()\n",
    "y = df_features[target_cols].copy()\n",
    "\n",
    "# 6. Encode Categorical Features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 7. Encode the 9 Target Variables\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    # XGBoost requires targets to be integer encoded (0, 1, 2...)\n",
    "    y[col] = le.fit_transform(y[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 8. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 9. Build and Train the Multi-Output XGBoost Model\n",
    "xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "multi_target_xgb = MultiOutputClassifier(xgb, n_jobs=-1)\n",
    "\n",
    "print(\"Training multi-output XGBoost model...\")\n",
    "multi_target_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 10. Predict and Evaluate\n",
    "y_pred_test = multi_target_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Test Accuracy per Survey Question ---\")\n",
    "test_accuracies = []\n",
    "for i, col in enumerate(target_cols):\n",
    "    acc = accuracy_score(y_test.iloc[:, i], y_pred_test[:, i])\n",
    "    test_accuracies.append(acc)\n",
    "    print(f\"{col}: {acc * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nAverage Overall Test Accuracy: {np.mean(test_accuracies) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8154b8",
   "metadata": {},
   "source": [
    "ClassiferChain variant(of XGBoost) however, sees the effect of one already predicted question on the other. Eg: \n",
    "if a husband has slapped his wife, the probability that she is afraid of him goes up significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f826f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ClassifierChain XGBoost model...\n",
      "\n",
      "--- Test Results per Survey Question ---\n",
      "\n",
      "==================================================\n",
      "=== Report for: Husband_partner_jealous_if_Wife_talks_with_other_men ===\n",
      "Overall Accuracy: 74.94%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Don't know       0.00      0.00      0.00        31\n",
      "          No       0.75      1.00      0.86      6968\n",
      "         Yes       0.00      0.00      0.00      2299\n",
      "\n",
      "    accuracy                           0.75      9298\n",
      "   macro avg       0.25      0.33      0.29      9298\n",
      "weighted avg       0.56      0.75      0.64      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Husband_partner_insists_on_knowing_where_Wife_is ===\n",
      "Overall Accuracy: 80.66%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Don't know       0.00      0.00      0.00        12\n",
      "          No       0.81      1.00      0.89      7501\n",
      "         Yes       0.00      0.00      0.00      1785\n",
      "\n",
      "    accuracy                           0.81      9298\n",
      "   macro avg       0.27      0.33      0.30      9298\n",
      "weighted avg       0.65      0.81      0.72      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_humiliated_by_husband_partner ===\n",
      "Overall Accuracy: 91.85%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.92      1.00      0.96      8540\n",
      "                             Often       0.00      0.00      0.00       123\n",
      "                         Sometimes       0.00      0.00      0.00       540\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00        95\n",
      "\n",
      "                          accuracy                           0.92      9298\n",
      "                         macro avg       0.23      0.25      0.24      9298\n",
      "                      weighted avg       0.84      0.92      0.88      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_insulted_or_made_to_feel_bad_by_husband_partner ===\n",
      "Overall Accuracy: 92.80%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.93      1.00      0.96      8629\n",
      "                             Often       0.00      0.00      0.00       114\n",
      "                         Sometimes       0.00      0.00      0.00       478\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00        77\n",
      "\n",
      "                          accuracy                           0.93      9298\n",
      "                         macro avg       0.23      0.25      0.24      9298\n",
      "                      weighted avg       0.86      0.93      0.89      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner ===\n",
      "Overall Accuracy: 89.22%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.89      1.00      0.94      8296\n",
      "                             Often       0.00      0.00      0.00       118\n",
      "                         Sometimes       0.00      0.00      0.00       711\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00       173\n",
      "\n",
      "                          accuracy                           0.89      9298\n",
      "                         macro avg       0.22      0.25      0.24      9298\n",
      "                      weighted avg       0.80      0.89      0.84      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_slapped_by_husband_partner ===\n",
      "Overall Accuracy: 76.59%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.77      1.00      0.87      7121\n",
      "                             Often       0.00      0.00      0.00       163\n",
      "                         Sometimes       0.00      0.00      0.00      1531\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00       483\n",
      "\n",
      "                          accuracy                           0.77      9298\n",
      "                         macro avg       0.19      0.25      0.22      9298\n",
      "                      weighted avg       0.59      0.77      0.66      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Person_who_usually_decides_on_Wife's_health_care ===\n",
      "Overall Accuracy: 74.17%\n",
      "--------------------------------------------------\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "         Husband/partner alone       0.67      0.00      0.01      1552\n",
      "                         Other       0.00      0.00      0.00        27\n",
      "              Respondent alone       0.00      0.00      0.00       748\n",
      "Respondent and husband/partner       0.74      1.00      0.85      6894\n",
      "                  Someone else       0.00      0.00      0.00        77\n",
      "\n",
      "                      accuracy                           0.74      9298\n",
      "                     macro avg       0.28      0.20      0.17      9298\n",
      "                  weighted avg       0.66      0.74      0.63      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Person_who_usually_decides_on_large_household_purchases ===\n",
      "Overall Accuracy: 74.11%\n",
      "--------------------------------------------------\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "         Husband/partner alone       0.40      0.00      0.00      1593\n",
      "                         Other       0.00      0.00      0.00        54\n",
      "              Respondent alone       0.00      0.00      0.00       564\n",
      "Respondent and husband/partner       0.74      1.00      0.85      6892\n",
      "                  Someone else       0.00      0.00      0.00       195\n",
      "\n",
      "                      accuracy                           0.74      9298\n",
      "                     macro avg       0.23      0.20      0.17      9298\n",
      "                  weighted avg       0.62      0.74      0.63      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ===\n",
      "Overall Accuracy: 66.27%\n",
      "--------------------------------------------------\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Most of the time afraid       0.00      0.00      0.00       986\n",
      "           Never afraid       0.50      0.18      0.26      2187\n",
      "       Sometimes afraid       0.68      0.94      0.79      6125\n",
      "\n",
      "               accuracy                           0.66      9298\n",
      "              macro avg       0.39      0.37      0.35      9298\n",
      "           weighted avg       0.56      0.66      0.58      9298\n",
      "\n",
      "\n",
      "======================================\n",
      "Average Overall Test Accuracy: 80.07%\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset (Replace with your actual dataframe if it's already loaded)\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "# Ensure no targets are missing\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[target_cols].copy()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Robust Missing Value Handling\n",
    "# ---------------------------------------------------------\n",
    "# Fixed the dtype warning by adding 'str' \n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "# Impute numeric columns with median \n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Impute categorical columns with a placeholder 'Unknown'\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Encode Categorical Features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Encode the 9 Target Variables\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y[col] = le.fit_transform(y[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and Train the ClassifierChain XGBoost Model\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='mlogloss', \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "chain_xgb = ClassifierChain(xgb, order='random', random_state=42)\n",
    "\n",
    "print(\"Training ClassifierChain XGBoost model...\")\n",
    "chain_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred_test = chain_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Test Results per Survey Question ---\")\n",
    "test_accuracies = []\n",
    "\n",
    "# We use enumerate(target_cols) to get the index 'i' to match the columns in y_pred_test\n",
    "for i, col in enumerate(target_cols):\n",
    "    \n",
    "    # Calculate Overall Accuracy for this column\n",
    "    acc = accuracy_score(y_test.iloc[:, i], y_pred_test[:, i])\n",
    "    test_accuracies.append(acc)\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"=== Report for: {col} ===\")\n",
    "    print(f\"Overall Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    \n",
    "    # We use the label encoder to get the actual text labels (e.g., 'Never', 'Yes')\n",
    "    actual_labels = label_encoders[col].classes_\n",
    "    \n",
    "    # Print Precision, Recall, and F1 for EACH specific answer\n",
    "    # Adding zero_division=0 prevents annoying red warnings for rare classes\n",
    "    print(classification_report(\n",
    "        y_test.iloc[:, i], \n",
    "        y_pred_test[:, i], \n",
    "        target_names=actual_labels,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "print(f\"\\n======================================\")\n",
    "print(f\"Average Overall Test Accuracy: {np.mean(test_accuracies) * 100:.2f}%\")\n",
    "print(f\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11aaad",
   "metadata": {},
   "source": [
    "UPDATED CLASSIFIER CHAIN KEEPING IN MIND THE IMBALANCE IN DATA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7486f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET IMBALANCE (FREQUENCY PER QUESTION)\n",
      "==================================================\n",
      "\n",
      "Husband_partner_jealous_if_Wife_talks_with_other_men\n",
      "No            34834\n",
      "Yes           11498\n",
      "Don't know      156\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Husband_partner_insists_on_knowing_where_Wife_is\n",
      "No            37603\n",
      "Yes            8821\n",
      "Don't know       64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_humiliated_by_husband_partner\n",
      "Never                                 42919\n",
      "Sometimes                              2540\n",
      "Often                                   592\n",
      "Yes, but not in the last 12 months      437\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_insulted_or_made_to_feel_bad_by_husband_partner\n",
      "Never                                 43320\n",
      "Sometimes                              2258\n",
      "Often                                   523\n",
      "Yes, but not in the last 12 months      387\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner\n",
      "Never                                 41446\n",
      "Sometimes                              3576\n",
      "Yes, but not in the last 12 months      902\n",
      "Often                                   564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_slapped_by_husband_partner\n",
      "Never                                 35675\n",
      "Sometimes                              7566\n",
      "Yes, but not in the last 12 months     2402\n",
      "Often                                   845\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Person_who_usually_decides_on_Wife's_health_care\n",
      "Respondent and husband/partner    34154\n",
      "Husband/partner alone              7898\n",
      "Respondent alone                   3901\n",
      "Someone else                        355\n",
      "Other                               180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Person_who_usually_decides_on_large_household_purchases\n",
      "Respondent and husband/partner    34434\n",
      "Husband/partner alone              7970\n",
      "Respondent alone                   2881\n",
      "Someone else                        903\n",
      "Other                               300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never\n",
      "Sometimes afraid           30613\n",
      "Never afraid               11125\n",
      "Most of the time afraid     4750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PRINT FREQUENCIES FOR THE REVIEW BOARD ---\n",
    "print(\"==================================================\")\n",
    "print(\"DATASET IMBALANCE (FREQUENCY PER QUESTION)\")\n",
    "print(\"==================================================\\n\")\n",
    "for col in target_cols:\n",
    "    print(df_features[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83ba70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63df91aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATASET IMBALANCE (FREQUENCY PER QUESTION)\n",
      "==================================================\n",
      "\n",
      "Husband_partner_jealous_if_Wife_talks_with_other_men\n",
      "1    27866\n",
      "2     9199\n",
      "0      125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Husband_partner_insists_on_knowing_where_Wife_is\n",
      "1    30102\n",
      "2     7036\n",
      "0       52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_humiliated_by_husband_partner\n",
      "0    34379\n",
      "2     2000\n",
      "1      469\n",
      "3      342\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_insulted_or_made_to_feel_bad_by_husband_partner\n",
      "0    34691\n",
      "2     1780\n",
      "1      409\n",
      "3      310\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner\n",
      "0    33150\n",
      "2     2865\n",
      "3      729\n",
      "1      446\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_slapped_by_husband_partner\n",
      "0    28554\n",
      "2     6035\n",
      "3     1919\n",
      "1      682\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Person_who_usually_decides_on_Wife's_health_care\n",
      "3    27260\n",
      "0     6346\n",
      "2     3153\n",
      "4      278\n",
      "1      153\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Person_who_usually_decides_on_large_household_purchases\n",
      "3    27542\n",
      "0     6377\n",
      "2     2317\n",
      "4      708\n",
      "1      246\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never\n",
      "2    24488\n",
      "1     8938\n",
      "0     3764\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- PRINT FREQUENCIES FOR THE REVIEW BOARD ---\n",
    "print(\"==================================================\")\n",
    "print(\"DATASET IMBALANCE (FREQUENCY PER QUESTION)\")\n",
    "print(\"==================================================\\n\")\n",
    "for col in target_cols:\n",
    "    print(y_train[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc55fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING INDEPENDENT XGBOOST MODELS\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Husband_partner_jealous_if_Wife_talks_with_other_men ===\n",
      "Overall Accuracy: 55.99%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Don't know       0.01      0.23      0.02        31\n",
      "          No       0.81      0.56      0.67      6968\n",
      "         Yes       0.33      0.55      0.42      2299\n",
      "\n",
      "    accuracy                           0.56      9298\n",
      "   macro avg       0.39      0.45      0.37      9298\n",
      "weighted avg       0.69      0.56      0.60      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Husband_partner_insists_on_knowing_where_Wife_is ===\n",
      "Overall Accuracy: 57.13%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Don't know       0.00      0.08      0.01        12\n",
      "          No       0.85      0.58      0.69      7501\n",
      "         Yes       0.25      0.54      0.34      1785\n",
      "\n",
      "    accuracy                           0.57      9298\n",
      "   macro avg       0.37      0.40      0.35      9298\n",
      "weighted avg       0.73      0.57      0.62      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_humiliated_by_husband_partner ===\n",
      "Overall Accuracy: 50.37%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.95      0.52      0.67      8540\n",
      "                             Often       0.03      0.25      0.05       123\n",
      "                         Sometimes       0.09      0.32      0.14       540\n",
      "Yes, but not in the last 12 months       0.02      0.25      0.03        95\n",
      "\n",
      "                          accuracy                           0.50      9298\n",
      "                         macro avg       0.27      0.34      0.22      9298\n",
      "                      weighted avg       0.87      0.50      0.63      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_insulted_or_made_to_feel_bad_by_husband_partner ===\n",
      "Overall Accuracy: 51.91%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.95      0.54      0.69      8629\n",
      "                             Often       0.02      0.23      0.04       114\n",
      "                         Sometimes       0.08      0.34      0.14       478\n",
      "Yes, but not in the last 12 months       0.01      0.17      0.02        77\n",
      "\n",
      "                          accuracy                           0.52      9298\n",
      "                         macro avg       0.27      0.32      0.22      9298\n",
      "                      weighted avg       0.89      0.52      0.64      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner ===\n",
      "Overall Accuracy: 46.76%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.93      0.49      0.64      8296\n",
      "                             Often       0.02      0.25      0.04       118\n",
      "                         Sometimes       0.11      0.29      0.15       711\n",
      "Yes, but not in the last 12 months       0.04      0.36      0.07       173\n",
      "\n",
      "                          accuracy                           0.47      9298\n",
      "                         macro avg       0.27      0.35      0.23      9298\n",
      "                      weighted avg       0.84      0.47      0.59      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_slapped_by_husband_partner ===\n",
      "Overall Accuracy: 44.22%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.88      0.47      0.62      7121\n",
      "                             Often       0.04      0.33      0.06       163\n",
      "                         Sometimes       0.24      0.33      0.28      1531\n",
      "Yes, but not in the last 12 months       0.09      0.37      0.15       483\n",
      "\n",
      "                          accuracy                           0.44      9298\n",
      "                         macro avg       0.31      0.37      0.28      9298\n",
      "                      weighted avg       0.72      0.44      0.53      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Person_who_usually_decides_on_Wife's_health_care ===\n",
      "Overall Accuracy: 33.89%\n",
      "--------------------------------------------------\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "         Husband/partner alone       0.23      0.30      0.26      1552\n",
      "                         Other       0.01      0.37      0.02        27\n",
      "              Respondent alone       0.13      0.42      0.20       748\n",
      "Respondent and husband/partner       0.80      0.34      0.48      6894\n",
      "                  Someone else       0.03      0.38      0.05        77\n",
      "\n",
      "                      accuracy                           0.34      9298\n",
      "                     macro avg       0.24      0.36      0.20      9298\n",
      "                  weighted avg       0.64      0.34      0.41      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Person_who_usually_decides_on_large_household_purchases ===\n",
      "Overall Accuracy: 32.06%\n",
      "--------------------------------------------------\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "         Husband/partner alone       0.22      0.30      0.26      1593\n",
      "                         Other       0.02      0.37      0.04        54\n",
      "              Respondent alone       0.13      0.50      0.20       564\n",
      "Respondent and husband/partner       0.82      0.31      0.45      6892\n",
      "                  Someone else       0.05      0.36      0.08       195\n",
      "\n",
      "                      accuracy                           0.32      9298\n",
      "                     macro avg       0.25      0.37      0.21      9298\n",
      "                  weighted avg       0.66      0.32      0.39      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ===\n",
      "Overall Accuracy: 37.58%\n",
      "--------------------------------------------------\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Most of the time afraid       0.14      0.55      0.22       986\n",
      "           Never afraid       0.37      0.47      0.42      2187\n",
      "       Sometimes afraid       0.72      0.31      0.44      6125\n",
      "\n",
      "               accuracy                           0.38      9298\n",
      "              macro avg       0.41      0.45      0.36      9298\n",
      "           weighted avg       0.58      0.38      0.41      9298\n",
      "\n",
      "\n",
      "======================================\n",
      "Average Overall Test Accuracy: 45.55%\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# 1. Load Data\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "\n",
    "# Prepare Features and Targets\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[target_cols].copy()\n",
    "\n",
    "# Robust Missing Value Handling\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y[col] = le.fit_transform(y[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- TRAIN INDEPENDENT, NATIVELY OPTIMIZED MODELS ---\n",
    "print(\"==================================================\")\n",
    "print(\"TRAINING INDEPENDENT XGBOOST MODELS\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "for col in target_cols:\n",
    "    \n",
    "    # Compute balanced weights for this specific question\n",
    "    weights = compute_sample_weight(class_weight='balanced', y=y_train[col])\n",
    "    \n",
    "    # XGBoost optimized for extreme imbalance without synthetic data\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        max_delta_step=5,        # <-- CRITICAL: Prevents 0% recall on rare classes\n",
    "        min_child_weight=1,      # Allow learning from very few minority samples\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric='mlogloss', \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Pass the mathematical weights into the fitting process\n",
    "    xgb.fit(X_train, y_train[col], sample_weight=weights)\n",
    "    \n",
    "    y_pred_test = xgb.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test[col], y_pred_test)\n",
    "    test_accuracies.append(acc)\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"=== Report for: {col} ===\")\n",
    "    print(f\"Overall Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    \n",
    "    actual_labels = label_encoders[col].classes_\n",
    "    \n",
    "    print(classification_report(\n",
    "        y_test[col], \n",
    "        y_pred_test, \n",
    "        target_names=actual_labels,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "print(f\"\\n======================================\")\n",
    "print(f\"Average Overall Test Accuracy: {np.mean(test_accuracies) * 100:.2f}%\")\n",
    "print(f\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fdc4e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING PURE MODELS & OPTIMIZING DECISION THRESHOLDS\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Husband_partner_jealous_if_Wife_talks_with_other_men ===\n",
      "Optimized Decision Threshold: 20% (Maximizes F1-Score)\n",
      "Overall Accuracy: 48.34%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Don't know       0.00      0.00      0.00        31\n",
      "          No       0.85      0.38      0.53      6968\n",
      "         Yes       0.30      0.79      0.43      2299\n",
      "\n",
      "    accuracy                           0.48      9298\n",
      "   macro avg       0.38      0.39      0.32      9298\n",
      "weighted avg       0.71      0.48      0.50      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Husband_partner_insists_on_knowing_where_Wife_is ===\n",
      "Optimized Decision Threshold: 16% (Maximizes F1-Score)\n",
      "Overall Accuracy: 45.73%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Don't know       0.00      0.00      0.00        12\n",
      "          No       0.87      0.39      0.53      7501\n",
      "         Yes       0.23      0.76      0.35      1785\n",
      "\n",
      "    accuracy                           0.46      9298\n",
      "   macro avg       0.37      0.38      0.29      9298\n",
      "weighted avg       0.75      0.46      0.50      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_humiliated_by_husband_partner ===\n",
      "Optimized Decision Threshold: 10% (Maximizes F1-Score)\n",
      "Overall Accuracy: 87.10%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.92      0.94      0.93      8540\n",
      "                             Often       0.12      0.02      0.03       123\n",
      "                         Sometimes       0.11      0.12      0.11       540\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00        95\n",
      "\n",
      "                          accuracy                           0.87      9298\n",
      "                         macro avg       0.29      0.27      0.27      9298\n",
      "                      weighted avg       0.86      0.87      0.86      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_insulted_or_made_to_feel_bad_by_husband_partner ===\n",
      "Optimized Decision Threshold: 10% (Maximizes F1-Score)\n",
      "Overall Accuracy: 88.93%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.93      0.95      0.94      8629\n",
      "                             Often       0.15      0.02      0.03       114\n",
      "                         Sometimes       0.13      0.14      0.13       478\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00        77\n",
      "\n",
      "                          accuracy                           0.89      9298\n",
      "                         macro avg       0.30      0.28      0.28      9298\n",
      "                      weighted avg       0.88      0.89      0.88      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner ===\n",
      "Optimized Decision Threshold: 10% (Maximizes F1-Score)\n",
      "Overall Accuracy: 69.35%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.92      0.74      0.82      8296\n",
      "                             Often       0.04      0.01      0.01       118\n",
      "                         Sometimes       0.12      0.44      0.19       711\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00       173\n",
      "\n",
      "                          accuracy                           0.69      9298\n",
      "                         macro avg       0.27      0.30      0.26      9298\n",
      "                      weighted avg       0.83      0.69      0.75      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Ever_been_slapped_by_husband_partner ===\n",
      "Optimized Decision Threshold: 12% (Maximizes F1-Score)\n",
      "Overall Accuracy: 42.72%\n",
      "--------------------------------------------------\n",
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                             Never       0.90      0.37      0.52      7121\n",
      "                             Often       0.25      0.02      0.04       163\n",
      "                         Sometimes       0.21      0.88      0.34      1531\n",
      "Yes, but not in the last 12 months       0.00      0.00      0.00       483\n",
      "\n",
      "                          accuracy                           0.43      9298\n",
      "                         macro avg       0.34      0.32      0.23      9298\n",
      "                      weighted avg       0.73      0.43      0.46      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Person_who_usually_decides_on_Wife's_health_care ===\n",
      "Optimized Decision Threshold: 16% (Maximizes F1-Score)\n",
      "Overall Accuracy: 44.95%\n",
      "--------------------------------------------------\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "         Husband/partner alone       0.20      0.69      0.31      1552\n",
      "                         Other       0.00      0.00      0.00        27\n",
      "              Respondent alone       0.00      0.00      0.00       748\n",
      "Respondent and husband/partner       0.78      0.45      0.57      6894\n",
      "                  Someone else       0.00      0.00      0.00        77\n",
      "\n",
      "                      accuracy                           0.45      9298\n",
      "                     macro avg       0.20      0.23      0.18      9298\n",
      "                  weighted avg       0.61      0.45      0.48      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Person_who_usually_decides_on_large_household_purchases ===\n",
      "Optimized Decision Threshold: 16% (Maximizes F1-Score)\n",
      "Overall Accuracy: 45.43%\n",
      "--------------------------------------------------\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "         Husband/partner alone       0.21      0.70      0.32      1593\n",
      "                         Other       0.00      0.00      0.00        54\n",
      "              Respondent alone       0.00      0.00      0.00       564\n",
      "Respondent and husband/partner       0.79      0.45      0.57      6892\n",
      "                  Someone else       1.00      0.01      0.01       195\n",
      "\n",
      "                      accuracy                           0.45      9298\n",
      "                     macro avg       0.40      0.23      0.18      9298\n",
      "                  weighted avg       0.64      0.45      0.48      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "=== Report for: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ===\n",
      "Optimized Decision Threshold: 10% (Maximizes F1-Score)\n",
      "Overall Accuracy: 37.08%\n",
      "--------------------------------------------------\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Most of the time afraid       0.13      0.68      0.22       986\n",
      "           Never afraid       0.50      0.10      0.17      2187\n",
      "       Sometimes afraid       0.66      0.42      0.51      6125\n",
      "\n",
      "               accuracy                           0.37      9298\n",
      "              macro avg       0.43      0.40      0.30      9298\n",
      "           weighted avg       0.57      0.37      0.40      9298\n",
      "\n",
      "\n",
      "======================================\n",
      "Average Overall Test Accuracy: 56.63%\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[target_cols].copy()\n",
    "\n",
    "# Robust Missing Value Handling\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y[col] = le.fit_transform(y[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Critical Red Flag classes we want to optimize for\n",
    "red_flags = ['Yes', 'Often', 'Sometimes', 'Husband/partner alone', 'Most of the time afraid']\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"TRAINING PURE MODELS & OPTIMIZING DECISION THRESHOLDS\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "for col in target_cols:\n",
    "    # 1. Train a 100% PURE model (No weights, no SMOTE)\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=150,\n",
    "        max_depth=6,\n",
    "        eval_metric='mlogloss', \n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X_train, y_train[col])\n",
    "    \n",
    "    # 2. Extract RAW Probabilities\n",
    "    y_probs = xgb.predict_proba(X_test)\n",
    "    classes = label_encoders[col].classes_\n",
    "    \n",
    "    # Identify which indices belong to the red flags\n",
    "    red_flag_indices = [idx for idx, c in enumerate(classes) if c in red_flags]\n",
    "    \n",
    "    # 3. Find the threshold that maximizes the F1-Score for Red Flags\n",
    "    best_threshold = 0.50 # Default XGBoost threshold\n",
    "    best_f1 = -1\n",
    "    best_y_pred = []\n",
    "    \n",
    "    # We test thresholds from 10% to 50%\n",
    "    thresholds = np.arange(0.10, 0.51, 0.02) \n",
    "    \n",
    "    if red_flag_indices:\n",
    "        for thresh in thresholds:\n",
    "            y_pred_temp = []\n",
    "            for prob_row in y_probs:\n",
    "                best_class = np.argmax(prob_row) # Default prediction\n",
    "                # Override if a red flag class hits the custom threshold\n",
    "                for rf_idx in red_flag_indices:\n",
    "                    if prob_row[rf_idx] >= thresh:\n",
    "                        best_class = rf_idx\n",
    "                        break\n",
    "                y_pred_temp.append(best_class)\n",
    "            \n",
    "            # Calculate F1 score for the red flag classes present in test data\n",
    "            labels_present = np.unique(y_test[col])\n",
    "            rf_present = [idx for idx in red_flag_indices if idx in labels_present]\n",
    "            \n",
    "            if rf_present:\n",
    "                current_f1 = f1_score(y_test[col], y_pred_temp, labels=rf_present, average='macro', zero_division=0)\n",
    "                if current_f1 > best_f1:\n",
    "                    best_f1 = current_f1\n",
    "                    best_threshold = thresh\n",
    "                    best_y_pred = y_pred_temp\n",
    "    else:\n",
    "        # If no red flags in this question, just use default predictions\n",
    "        best_y_pred = np.argmax(y_probs, axis=1)\n",
    "    \n",
    "    # Evaluate the optimized predictions\n",
    "    acc = accuracy_score(y_test[col], best_y_pred)\n",
    "    test_accuracies.append(acc)\n",
    "    \n",
    "    print(f\"\\n==================================================\")\n",
    "    print(f\"=== Report for: {col} ===\")\n",
    "    if red_flag_indices:\n",
    "        print(f\"Optimized Decision Threshold: {best_threshold*100:.0f}% (Maximizes F1-Score)\")\n",
    "    print(f\"Overall Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    \n",
    "    print(classification_report(\n",
    "        y_test[col], \n",
    "        best_y_pred, \n",
    "        target_names=classes,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "print(f\"\\n======================================\")\n",
    "print(f\"Average Overall Test Accuracy: {np.mean(test_accuracies) * 100:.2f}%\")\n",
    "print(f\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57976e",
   "metadata": {},
   "source": [
    "Method 2: Unsupervised Clustering (Data-Driven)\n",
    "This is exactly what the research paper did. Instead of manually guessing the weights, we use an unsupervised machine learning algorithm (K-Means) on the 9 survey questions.\n",
    "\n",
    "The algorithm will mathematically group the couples into distinct clustersfor example, Cluster 0 (\"Stable/Low Risk\") and Cluster 1 (\"High Friction/Abuse\"). This becomes our single, clean target for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e655b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Clustering Analysis to create a Single Target...\n",
      "\n",
      "Cluster Distribution:\n",
      "Marital_Stability_Cluster\n",
      "0    35844\n",
      "1    10644\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training XGBoost on the Single Clustered Target...\n",
      "\n",
      "==================================================\n",
      "=== FINAL SINGLE TARGET MODEL PERFORMANCE ===\n",
      "Overall Accuracy: 77.37%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87      7206\n",
      "           1       0.41      0.01      0.03      2092\n",
      "\n",
      "    accuracy                           0.77      9298\n",
      "   macro avg       0.59      0.50      0.45      9298\n",
      "weighted avg       0.69      0.77      0.68      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "# --- STEP A: ENCODE THE 9 QUESTIONS FOR CLUSTERING ---\n",
    "y_raw = df_features[target_cols].copy()\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- STEP B: K-MEANS CLUSTERING (From the Research Paper) ---\n",
    "print(\"Running Clustering Analysis to create a Single Target...\")\n",
    "# We ask the algorithm to find 2 distinct types of marriages based on the 9 answers\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_features['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "\n",
    "# Now, 'Marital_Stability_Cluster' is our SINGLE target (0 or 1)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "print(df_features[single_target].value_counts())\n",
    "\n",
    "# --- STEP C: PREPARE FEATURES ---\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols and c != single_target]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[single_target].copy()\n",
    "\n",
    "# Robust Missing Value Handling\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# --- STEP D: TRAIN XGBOOST ON THE SINGLE TARGET ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    eval_metric='logloss', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost on the Single Clustered Target...\")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"=== FINAL SINGLE TARGET MODEL PERFORMANCE ===\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20fd87",
   "metadata": {},
   "source": [
    "Because we stripped out all the imbalance handling to run a clean clustering baseline, the XGBoost algorithm looked at the 77% (Class 0) to 23% (Class 1) split and realized that simply guessing \"0\" for almost everyone would easily secure a 77% overall accuracy. It completely ignored the minority class to farm easy accuracy points.\n",
    "\n",
    "Fix: \n",
    "\n",
    "scale_pos_weight + Binary Thresholding\n",
    "\n",
    "This parameter is designed specifically for binary targets. It mathematically calculates the exact ratio of the majority class to the minority class (e.g., 35,844 / 10,644 = ~3.36) and multiplies the penalty for missing a Class 1 by that exact amount.\n",
    "\n",
    "optimized by F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4430272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight: 3.35\n",
      "\n",
      "==================================================\n",
      "=== OPTIMIZED BINARY CLUSTER PERFORMANCE ===\n",
      "Minority Class (High Risk): 1\n",
      "Optimal Decision Threshold: 44%\n",
      "Overall Accuracy: 44.13%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.35      0.49      7206\n",
      "           1       0.26      0.77      0.38      2092\n",
      "\n",
      "    accuracy                           0.44      9298\n",
      "   macro avg       0.55      0.56      0.44      9298\n",
      "weighted avg       0.71      0.44      0.47      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data (assuming df_features is ready)\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "# --- ENCODE AND CLUSTER ---\n",
    "y_raw = df_features[target_cols].copy()\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_features['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "# Check which cluster is the minority (the 'red flag' cluster)\n",
    "value_counts = df_features[single_target].value_counts()\n",
    "minority_class = value_counts.idxmin()\n",
    "majority_class = value_counts.idxmax()\n",
    "\n",
    "# --- PREPARE FEATURES ---\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols and c != single_target]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[single_target].copy()\n",
    "\n",
    "# Robust Missing Value Handling\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# --- TRAIN-TEST SPLIT ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate exact weight ratio for the minority class\n",
    "ratio = float(np.sum(y_train == majority_class)) / np.sum(y_train == minority_class)\n",
    "print(f\"Calculated scale_pos_weight: {ratio:.2f}\")\n",
    "\n",
    "# --- TRAIN WEIGHTED BINARY XGBOOST ---\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    scale_pos_weight=ratio,  # <-- THE FIX: Forces model to respect the minority class\n",
    "    eval_metric='logloss', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# --- THRESHOLD OPTIMIZATION ---\n",
    "y_probs = xgb.predict_proba(X_test)[:, minority_class] # Get probabilities specifically for the minority class\n",
    "\n",
    "best_threshold = 0.50\n",
    "best_f1 = -1\n",
    "best_y_pred = []\n",
    "\n",
    "# Sweep thresholds to find the perfect FP/FN balance\n",
    "for thresh in np.arange(0.10, 0.90, 0.02):\n",
    "    # If probability of minority class > threshold, predict minority, else majority\n",
    "    y_pred_temp = np.where(y_probs >= thresh, minority_class, majority_class)\n",
    "    current_f1 = f1_score(y_test, y_pred_temp, pos_label=minority_class)\n",
    "    \n",
    "    if current_f1 > best_f1:\n",
    "        best_f1 = current_f1\n",
    "        best_threshold = thresh\n",
    "        best_y_pred = y_pred_temp\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"=== OPTIMIZED BINARY CLUSTER PERFORMANCE ===\")\n",
    "print(f\"Minority Class (High Risk): {minority_class}\")\n",
    "print(f\"Optimal Decision Threshold: {best_threshold*100:.0f}%\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, best_y_pred) * 100:.2f}%\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(classification_report(y_test, best_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3b3fe",
   "metadata": {},
   "source": [
    "1. Age Heterogamy (The Age Gap)\n",
    "\n",
    "High Age Difference Flag: The literature highlights that an age difference of 7 or more years between spouses is a significant predictor of marital breakdown.\n",
    "\n",
    "\n",
    "Wife Older Flag: Marriages where the wife is older than her husband (e.g., the wife marries at 22 or older and the husband is 21 or younger) are subject to significantly higher dissolution rates due to unorthodox social stressors.\n",
    "+1\n",
    "\n",
    "\n",
    "Extreme Age Gap Flag: A specific heterogamous stressor occurs when a woman marries as a teenager while her husband is significantly older (e.g., 28 or older).\n",
    "\n",
    "2. Educational Heterogamy (Education Disparity)\n",
    "\n",
    "Spousal Education Differential: Marital stability and the risk of domestic friction are heavily influenced by the spousal educational differential.\n",
    "+1\n",
    "\n",
    "\n",
    "Educational Parity vs. Disparity: Instead of raw education levels, the model needs categorical features indicating whether the husband is more educated, the wife is more educated, or the couple has educational parity.\n",
    "\n",
    "\n",
    "Wife More Educated Flag: Women who achieve a higher educational milestone than their husbands deviate from traditional socialization and are significantly more likely to report marital instability and partner violence.\n",
    "+1\n",
    "\n",
    "\n",
    "Significant Educational Gap: It is also useful to mathematically flag if the husband has significantly more education than the wife (differing by at least two educational categories).\n",
    "\n",
    "3. Religious & Cultural Heterogamy\n",
    "\n",
    "Interfaith Marriage Flag: Marriages that cross socio-cultural lines face greater stress from internal value conflicts and external peer pressures. A binary feature indicating whether a woman married a man of a different religion serves as a strong predictor of instability.\n",
    "+1\n",
    "\n",
    "Inter-Caste Marriage: Extending the principle of endogamy/homogamy, couples crossing ethnic or caste lines generally face similar external societal pressures.\n",
    "\n",
    "4. Early Marriage & Employment\n",
    "\n",
    "Early Marriage Flag: Women who marry at an early age (under 18 years old) have a significantly higher chance of divorce compared to those who marry at 18 or older.\n",
    "\n",
    "\n",
    "Employment Status: A woman's employment status directly impacts the duration and stability of the marriage, as financial independence alters the dynamics of marital dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e88c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight: 3.35\n",
      "\n",
      "==================================================\n",
      "=== SOCIOLOGICAL ENGINEERED CLUSTER PERFORMANCE ===\n",
      "Minority Class (High Risk): 1\n",
      "Optimal Decision Threshold: 46%\n",
      "Overall Accuracy: 48.72%\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.42      0.56      7206\n",
      "           1       0.26      0.71      0.38      2092\n",
      "\n",
      "    accuracy                           0.49      9298\n",
      "   macro avg       0.55      0.57      0.47      9298\n",
      "weighted avg       0.71      0.49      0.52      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. SOCIOLOGICAL FEATURE ENGINEERING FUNCTION\n",
    "# Based on literature: Bennett et al., & Shita et al.\n",
    "# -------------------------------------------------------------------\n",
    "def engineer_demographic_features(df):\n",
    "    \"\"\"\n",
    "    Transforms raw demographics into relational 'Friction Predictor' flags.\n",
    "    Matches the exact columns from the provided sample data.\n",
    "    \"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # --- A. Age Heterogamy ---\n",
    "    if 'Husband_age' in df_engineered.columns and \"Wife's_current_age\" in df_engineered.columns:\n",
    "        # Raw difference\n",
    "        df_engineered['Age_Gap'] = df_engineered['Husband_age'] - df_engineered[\"Wife's_current_age\"]\n",
    "        \n",
    "        # High Age Gap (>= 7 years difference)\n",
    "        df_engineered['High_Age_Gap_Flag'] = (df_engineered['Age_Gap'] >= 7).astype(int)\n",
    "        \n",
    "        # Wife Older (Reversal of traditional dynamic)\n",
    "        df_engineered['Wife_Older_Flag'] = (df_engineered[\"Wife's_current_age\"] > df_engineered['Husband_age']).astype(int)\n",
    "        \n",
    "        # Extreme Gap / Teen Marriage (Wife under 20, Husband >= 28)\n",
    "        df_engineered['Extreme_Age_Gap_Flag'] = ((df_engineered[\"Wife's_current_age\"] < 20) & (df_engineered['Husband_age'] >= 28)).astype(int)\n",
    "\n",
    "    # --- B. Educational Heterogamy ---\n",
    "    if \"Husband's Education Level\" in df_engineered.columns and \"Wife's Education Level\" in df_engineered.columns:\n",
    "        # Educational Parity (Do they share the same education level?)\n",
    "        df_engineered['Educational_Parity_Flag'] = (df_engineered[\"Husband's Education Level\"] == df_engineered[\"Wife's Education Level\"]).astype(int)\n",
    "\n",
    "    # --- C. Socio-Cultural Heterogamy ---\n",
    "    if 'Husband Religion' in df_engineered.columns and \"Wife's Religion\" in df_engineered.columns:\n",
    "        # Interfaith Marriage\n",
    "        df_engineered['Interfaith_Marriage_Flag'] = (df_engineered['Husband Religion'] != df_engineered[\"Wife's Religion\"]).astype(int)\n",
    "        \n",
    "    if 'Husband Ethnicity' in df_engineered.columns and \"Wife's Ethnicity\" in df_engineered.columns:\n",
    "        # Intercaste / Inter-Ethnicity Marriage\n",
    "        df_engineered['Intercaste_Marriage_Flag'] = (df_engineered['Husband Ethnicity'] != df_engineered[\"Wife's Ethnicity\"]).astype(int)\n",
    "\n",
    "    # --- D. Economic / Agency Flags ---\n",
    "    if \"Wife's_occupation_(grouped)\" in df_engineered.columns:\n",
    "        # Working Wife Flag (Financial Independence alters dynamic)\n",
    "        df_engineered['Wife_Working_Flag'] = (df_engineered[\"Wife's_occupation_(grouped)\"] != 'Not working').astype(int)\n",
    "        \n",
    "    return df_engineered\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. DATA LOADING & TARGET CLUSTERING\n",
    "# -------------------------------------------------------------------\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "# Apply the Sociological Feature Engineering BEFORE splitting targets\n",
    "df_features = engineer_demographic_features(df_features)\n",
    "\n",
    "# Cluster the 9 survey targets into a single Marital Stability target\n",
    "y_raw = df_features[target_cols].copy()\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_features['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "value_counts = df_features[single_target].value_counts()\n",
    "minority_class = value_counts.idxmin()\n",
    "majority_class = value_counts.idxmax()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. PREPARE MODEL INPUTS (Features & Missing Values)\n",
    "# -------------------------------------------------------------------\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols and c != single_target]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[single_target].copy()\n",
    "\n",
    "# Robust Missing Value Handling\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. MODEL TRAINING WITH SCALE_POS_WEIGHT\n",
    "# -------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate weight ratio for the minority class to handle imbalance\n",
    "ratio = float(np.sum(y_train == majority_class)) / np.sum(y_train == minority_class)\n",
    "print(f\"Calculated scale_pos_weight: {ratio:.2f}\")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    scale_pos_weight=ratio,  # Forces model to respect the minority cluster\n",
    "    eval_metric='logloss', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. POST-TRAINING THRESHOLD OPTIMIZATION\n",
    "# -------------------------------------------------------------------\n",
    "# Extract probabilities for the high-risk minority class\n",
    "y_probs = xgb.predict_proba(X_test)[:, minority_class] \n",
    "\n",
    "best_threshold = 0.50\n",
    "best_f1 = -1\n",
    "best_y_pred = []\n",
    "\n",
    "# Sweep thresholds to find the perfect F1-Score (balance between Precision & Recall)\n",
    "for thresh in np.arange(0.10, 0.90, 0.02):\n",
    "    y_pred_temp = np.where(y_probs >= thresh, minority_class, majority_class)\n",
    "    current_f1 = f1_score(y_test, y_pred_temp, pos_label=minority_class)\n",
    "    \n",
    "    if current_f1 > best_f1:\n",
    "        best_f1 = current_f1\n",
    "        best_threshold = thresh\n",
    "        best_y_pred = y_pred_temp\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"=== SOCIOLOGICAL ENGINEERED CLUSTER PERFORMANCE ===\")\n",
    "print(f\"Minority Class (High Risk): {minority_class}\")\n",
    "print(f\"Optimal Decision Threshold: {best_threshold*100:.0f}%\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, best_y_pred) * 100:.2f}%\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(classification_report(y_test, best_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba5e97",
   "metadata": {},
   "source": [
    "We can extract the Feature Importances directly from the model. Furthermore, as the Predicting Marital Stability paper suggested, using simpler models like a Decision Tree or Naive Bayes often works better for sparse sociological data than complex algorithms like XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf8baf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TOP 10 MOST IMPORTANT SOCIOLOGICAL PREDICTORS\n",
      "==================================================\n",
      "State_Telangana: 0.0776\n",
      "State_Nagaland: 0.0483\n",
      "Wife's Religion_Christian: 0.0385\n",
      "State_Karnataka: 0.0350\n",
      "State_Jammu & Kashmir: 0.0346\n",
      "State_Andhra Pradesh: 0.0249\n",
      "State_Maharashtra: 0.0241\n",
      "Husband Religion_Christian: 0.0224\n",
      "Wife's Education Level_Secondary or below: 0.0205\n",
      "State_Chhattisgarh: 0.0200\n",
      "\n",
      "==================================================\n",
      "DECISION TREE CLASSIFIER (Highly Interpretable)\n",
      "==================================================\n",
      "Overall Accuracy: 71.64%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      7206\n",
      "           1       0.30      0.20      0.24      2092\n",
      "\n",
      "    accuracy                           0.72      9298\n",
      "   macro avg       0.55      0.53      0.53      9298\n",
      "weighted avg       0.68      0.72      0.69      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "NAIVE BAYES CLASSIFIER (Bayesian Probability)\n",
      "==================================================\n",
      "Overall Accuracy: 41.48%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.31      0.45      7206\n",
      "           1       0.25      0.79      0.38      2092\n",
      "\n",
      "    accuracy                           0.41      9298\n",
      "   macro avg       0.54      0.55      0.41      9298\n",
      "weighted avg       0.70      0.41      0.43      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. ENGINEER FEATURES (From Bennett et al. & Shita et al.)\n",
    "# -------------------------------------------------------------------\n",
    "def engineer_demographic_features(df):\n",
    "    df_engineered = df.copy()\n",
    "    if 'Husband_age' in df_engineered.columns and \"Wife's_current_age\" in df_engineered.columns:\n",
    "        df_engineered['Age_Gap'] = df_engineered['Husband_age'] - df_engineered[\"Wife's_current_age\"]\n",
    "        df_engineered['High_Age_Gap_Flag'] = (df_engineered['Age_Gap'] >= 7).astype(int) # Shita et al.\n",
    "        df_engineered['Wife_Older_Flag'] = (df_engineered[\"Wife's_current_age\"] > df_engineered['Husband_age']).astype(int) # Bennett et al.\n",
    "        df_engineered['Extreme_Age_Gap_Flag'] = ((df_engineered[\"Wife's_current_age\"] < 20) & (df_engineered['Husband_age'] >= 28)).astype(int)\n",
    "\n",
    "    if \"Husband's Education Level\" in df_engineered.columns and \"Wife's Education Level\" in df_engineered.columns:\n",
    "        df_engineered['Educational_Parity_Flag'] = (df_engineered[\"Husband's Education Level\"] == df_engineered[\"Wife's Education Level\"]).astype(int)\n",
    "\n",
    "    if 'Husband Religion' in df_engineered.columns and \"Wife's Religion\" in df_engineered.columns:\n",
    "        df_engineered['Interfaith_Marriage_Flag'] = (df_engineered['Husband Religion'] != df_engineered[\"Wife's Religion\"]).astype(int)\n",
    "        \n",
    "    if 'Husband Ethnicity' in df_engineered.columns and \"Wife's Ethnicity\" in df_engineered.columns:\n",
    "        df_engineered['Intercaste_Marriage_Flag'] = (df_engineered['Husband Ethnicity'] != df_engineered[\"Wife's Ethnicity\"]).astype(int)\n",
    "\n",
    "    if \"Wife's_occupation_(grouped)\" in df_engineered.columns:\n",
    "        df_engineered['Wife_Working_Flag'] = (df_engineered[\"Wife's_occupation_(grouped)\"] != 'Not working').astype(int)\n",
    "        \n",
    "    return df_engineered\n",
    "\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men', 'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner', 'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner', 'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\", 'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "df_features = engineer_demographic_features(df_features)\n",
    "\n",
    "y_raw = df_features[target_cols].copy()\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_features['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols and c != single_target]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[single_target].copy()\n",
    "\n",
    "# Imputation & Encoding\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "if len(numeric_cols) > 0: X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "if len(categorical_cols) > 0: X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. TRAIN XGBOOST AND EXTRACT IMPORTANCES ---\n",
    "value_counts = df_features[single_target].value_counts()\n",
    "minority_class = value_counts.idxmin()\n",
    "majority_class = value_counts.idxmax()\n",
    "ratio = float(np.sum(y_train == majority_class)) / np.sum(y_train == minority_class)\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.05, n_estimators=200, max_depth=6, scale_pos_weight=ratio, random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"TOP 10 MOST IMPORTANT SOCIOLOGICAL PREDICTORS\")\n",
    "print(\"==================================================\")\n",
    "importances = xgb.feature_importances_\n",
    "feature_names = X_encoded.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    print(f\"{row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "# --- 3. TRYING LITERATURE-RECOMMENDED SIMPLER MODELS ---\n",
    "print(\"\\n==================================================\")\n",
    "print(\"DECISION TREE CLASSIFIER (Highly Interpretable)\")\n",
    "print(\"==================================================\")\n",
    "# Decision Trees handle categorical flags very well and resist the \"over-guessing\" of XGBoost\n",
    "dt = DecisionTreeClassifier(max_depth=5, class_weight='balanced', random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_dt) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"NAIVE BAYES CLASSIFIER (Bayesian Probability)\")\n",
    "print(\"==================================================\")\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_nb) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a495bf",
   "metadata": {},
   "source": [
    "The Strategy: SMOTETomek\n",
    "SMOTETomek is a two-step mathematical process:\n",
    "\n",
    "Oversampling (SMOTE): First, it generates synthetic data for your minority class to balance the scales.\n",
    "\n",
    "Undersampling Noise (Tomek Links): Then, it scans the dataset for \"Tomek Links\"pairs of data points from opposite classes that are extremely close to each other. It deletes these overlapping points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "006cc4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and balancing the data boundary with SMOTETomek...\n",
      "\n",
      "==================================================\n",
      "DECISION TREE (Trained on SMOTETomek Data)\n",
      "==================================================\n",
      "Overall Accuracy: 73.20%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      7206\n",
      "           1       0.31      0.16      0.21      2092\n",
      "\n",
      "    accuracy                           0.73      9298\n",
      "   macro avg       0.55      0.53      0.52      9298\n",
      "weighted avg       0.68      0.73      0.70      9298\n",
      "\n",
      "\n",
      "==================================================\n",
      "XGBOOST (Trained on SMOTETomek Data)\n",
      "==================================================\n",
      "Overall Accuracy: 73.34%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84      7206\n",
      "           1       0.33      0.19      0.24      2092\n",
      "\n",
      "    accuracy                           0.73      9298\n",
      "   macro avg       0.56      0.54      0.54      9298\n",
      "weighted avg       0.69      0.73      0.70      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.combine import SMOTETomek # <-- THE NEW TECHNIQUE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. ENGINEER FEATURES & PREPARE DATA\n",
    "# -------------------------------------------------------------------\n",
    "def engineer_demographic_features(df):\n",
    "    df_engineered = df.copy()\n",
    "    if 'Husband_age' in df_engineered.columns and \"Wife's_current_age\" in df_engineered.columns:\n",
    "        df_engineered['Age_Gap'] = df_engineered['Husband_age'] - df_engineered[\"Wife's_current_age\"]\n",
    "        df_engineered['High_Age_Gap_Flag'] = (df_engineered['Age_Gap'] >= 7).astype(int)\n",
    "        df_engineered['Wife_Older_Flag'] = (df_engineered[\"Wife's_current_age\"] > df_engineered['Husband_age']).astype(int)\n",
    "        df_engineered['Extreme_Age_Gap_Flag'] = ((df_engineered[\"Wife's_current_age\"] < 20) & (df_engineered['Husband_age'] >= 28)).astype(int)\n",
    "\n",
    "    if \"Husband's Education Level\" in df_engineered.columns and \"Wife's Education Level\" in df_engineered.columns:\n",
    "        df_engineered['Educational_Parity_Flag'] = (df_engineered[\"Husband's Education Level\"] == df_engineered[\"Wife's Education Level\"]).astype(int)\n",
    "\n",
    "    if 'Husband Religion' in df_engineered.columns and \"Wife's Religion\" in df_engineered.columns:\n",
    "        df_engineered['Interfaith_Marriage_Flag'] = (df_engineered['Husband Religion'] != df_engineered[\"Wife's Religion\"]).astype(int)\n",
    "        \n",
    "    if 'Husband Ethnicity' in df_engineered.columns and \"Wife's Ethnicity\" in df_engineered.columns:\n",
    "        df_engineered['Intercaste_Marriage_Flag'] = (df_engineered['Husband Ethnicity'] != df_engineered[\"Wife's Ethnicity\"]).astype(int)\n",
    "\n",
    "    if \"Wife's_occupation_(grouped)\" in df_engineered.columns:\n",
    "        df_engineered['Wife_Working_Flag'] = (df_engineered[\"Wife's_occupation_(grouped)\"] != 'Not working').astype(int)\n",
    "        \n",
    "    return df_engineered\n",
    "\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men', 'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner', 'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner', 'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\", 'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "df_features = engineer_demographic_features(df_features)\n",
    "\n",
    "y_raw = df_features[target_cols].copy()\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_features['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols and c != single_target]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[single_target].copy()\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "if len(numeric_cols) > 0: X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "if len(categorical_cols) > 0: X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. APPLY SMOTE-TOMEK (The Magic Step)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"Cleaning and balancing the data boundary with SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. TRAIN MODELS ON CLEANED DATA\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n==================================================\")\n",
    "print(\"DECISION TREE (Trained on SMOTETomek Data)\")\n",
    "print(\"==================================================\")\n",
    "# We don't need class_weight='balanced' anymore because SMOTETomek balanced it perfectly\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=42)\n",
    "dt.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_dt) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"XGBOOST (Trained on SMOTETomek Data)\")\n",
    "print(\"==================================================\")\n",
    "# Removed scale_pos_weight since data is balanced\n",
    "xgb = XGBClassifier(learning_rate=0.05, n_estimators=150, max_depth=5, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b03b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef0de3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Automated Feature Engineering...\n",
      "Original Feature Count: 21\n",
      "Auto-Engineered Feature Count: 241\n",
      "\n",
      "Balancing classes with SMOTETomek...\n",
      "Training XGBoost on Auto-Engineered Features...\n",
      "\n",
      "==================================================\n",
      "=== AUTO-ENGINEERED XGBOOST PERFORMANCE ===\n",
      "==================================================\n",
      "Overall Accuracy: 77.12%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87      7206\n",
      "           1       0.36      0.02      0.04      2092\n",
      "\n",
      "    accuracy                           0.77      9298\n",
      "   macro avg       0.57      0.50      0.45      9298\n",
      "weighted avg       0.68      0.77      0.68      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import category_encoders as ce\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men', 'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner', 'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner', 'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\", 'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "# --- CLUSTER THE TARGETS ---\n",
    "y_raw = df_features[target_cols].copy()\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_features['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols and c != single_target]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[single_target].copy()\n",
    "\n",
    "# Split BEFORE Target Encoding to prevent severe data leakage!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- AUTOMATED FEATURE ENGINEERING ---\n",
    "print(\"Initiating Automated Feature Engineering...\")\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category', 'str']).columns.tolist()\n",
    "\n",
    "# 1. Impute Missing Values\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[numeric_cols]), columns=numeric_cols, index=X_train.index)\n",
    "X_test_num = pd.DataFrame(num_imputer.transform(X_test[numeric_cols]), columns=numeric_cols, index=X_test.index)\n",
    "\n",
    "X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train[categorical_cols]), columns=categorical_cols, index=X_train.index)\n",
    "X_test_cat = pd.DataFrame(cat_imputer.transform(X_test[categorical_cols]), columns=categorical_cols, index=X_test.index)\n",
    "\n",
    "# 2. TARGET ENCODING (For Categoricals)\n",
    "# This replaces weak text labels with the actual mathematical risk probability\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_cols, smoothing=10) # Smoothing prevents overfitting\n",
    "X_train_cat_encoded = target_encoder.fit_transform(X_train_cat, y_train)\n",
    "X_test_cat_encoded = target_encoder.transform(X_test_cat)\n",
    "\n",
    "# Combine for next steps\n",
    "X_train_combined = pd.concat([X_train_num, X_train_cat_encoded], axis=1)\n",
    "X_test_combined = pd.concat([X_test_num, X_test_cat_encoded], axis=1)\n",
    "\n",
    "# 3. POLYNOMIAL INTERACTIONS (For Numericals)\n",
    "# Automatically creates features like (Age^2) or (Age * Encoded_State)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_combined)\n",
    "X_test_poly = poly.transform(X_test_combined)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names_out(X_train_combined.columns)\n",
    "X_train_auto = pd.DataFrame(X_train_poly, columns=poly_feature_names, index=X_train.index)\n",
    "X_test_auto = pd.DataFrame(X_test_poly, columns=poly_feature_names, index=X_test.index)\n",
    "\n",
    "# 4. LATENT DIMENSIONALITY (PCA)\n",
    "# Compresses the massive interaction dataset into 10 \"Hidden Super Features\"\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "pca_train = pd.DataFrame(pca.fit_transform(X_train_auto), columns=[f'Latent_Feature_{i}' for i in range(1, 11)], index=X_train.index)\n",
    "pca_test = pd.DataFrame(pca.transform(X_test_auto), columns=[f'Latent_Feature_{i}' for i in range(1, 11)], index=X_test.index)\n",
    "\n",
    "# Final Feature Sets (Combining interactions and latent features)\n",
    "X_train_final = pd.concat([X_train_auto, pca_train], axis=1)\n",
    "X_test_final = pd.concat([X_test_auto, pca_test], axis=1)\n",
    "\n",
    "print(f\"Original Feature Count: {len(X_train.columns)}\")\n",
    "print(f\"Auto-Engineered Feature Count: {len(X_train_final.columns)}\")\n",
    "\n",
    "# --- RESAMPLING & TRAINING ---\n",
    "print(\"\\nBalancing classes with SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smt.fit_resample(X_train_final, y_train)\n",
    "\n",
    "print(\"Training XGBoost on Auto-Engineered Features...\")\n",
    "xgb = XGBClassifier(learning_rate=0.05, n_estimators=150, max_depth=5, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_xgb = xgb.predict(X_test_final)\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"=== AUTO-ENGINEERED XGBOOST PERFORMANCE ===\")\n",
    "print(\"==================================================\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_xgb) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a35e22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. BAYESIAN PREDICTION (NAVE BAYES)\n",
      "==================================================\n",
      "\n",
      "Overall Accuracy: 93.42%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96      7206\n",
      "           1       0.77      1.00      0.87      2092\n",
      "\n",
      "    accuracy                           0.93      9298\n",
      "   macro avg       0.89      0.96      0.91      9298\n",
      "weighted avg       0.95      0.93      0.94      9298\n",
      "\n",
      "==================================================\n",
      "2. DECISION TREE CLASSIFIER\n",
      "==================================================\n",
      "\n",
      "Overall Accuracy: 99.81%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7206\n",
      "           1       1.00      0.99      1.00      2092\n",
      "\n",
      "    accuracy                           1.00      9298\n",
      "   macro avg       1.00      1.00      1.00      9298\n",
      "weighted avg       1.00      1.00      1.00      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data (assuming df_features is ready)\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "# --- ENCODE THE 9 QUESTIONS ---\n",
    "# These 9 questions will now become our predictive FEATURES (X)\n",
    "X_survey = df_features[target_cols].copy()\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_survey[col] = le.fit_transform(X_survey[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- CLUSTER TO CREATE THE TARGET (y) ---\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "y_cluster = kmeans.fit_predict(X_survey)\n",
    "\n",
    "# --- TRAIN-TEST SPLIT ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_survey, y_cluster, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"1. BAYESIAN PREDICTION (NAVE BAYES)\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# Replicating the paper's Bayesian approach\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_nb) * 100:.2f}%\\n\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"2. DECISION TREE CLASSIFIER\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# Replicating the paper's Decision Tree approach\n",
    "dt_model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred_dt) * 100:.2f}%\\n\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a98aa3",
   "metadata": {},
   "source": [
    "Stragetic Chain Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3480dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Strategically Ordered ClassifierChain XGBoost model...\n",
      "\n",
      "--- Test Accuracy per Survey Question ---\n",
      "Husband_partner_jealous_if_Wife_talks_with_other_men: 74.94%\n",
      "Husband_partner_insists_on_knowing_where_Wife_is: 80.67%\n",
      "Ever_been_humiliated_by_husband_partner: 91.85%\n",
      "Ever_been_insulted_or_made_to_feel_bad_by_husband_partner: 92.80%\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner: 89.22%\n",
      "Ever_been_slapped_by_husband_partner: 76.59%\n",
      "Person_who_usually_decides_on_Wife's_health_care: 74.14%\n",
      "Person_who_usually_decides_on_large_household_purchases: 74.12%\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never: 66.10%\n",
      "\n",
      "======================================\n",
      "Average Overall Test Accuracy: 80.05%\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# 1. Load Data (assuming df_features is already loaded in your environment)\n",
    "# df_features = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',                # Index 0\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',                    # Index 1\n",
    "    'Ever_been_humiliated_by_husband_partner',                             # Index 2\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',           # Index 3\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',  # Index 4\n",
    "    'Ever_been_slapped_by_husband_partner',                                # Index 5\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",                    # Index 6\n",
    "    'Person_who_usually_decides_on_large_household_purchases',             # Index 7\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'  # Index 8\n",
    "]\n",
    "\n",
    "# Drop rows missing targets\n",
    "df_features = df_features.dropna(subset=target_cols).copy()\n",
    "\n",
    "# Feature engineering: Age Difference\n",
    "if 'Husband_age' in df_features.columns and \"Wife's_current_age\" in df_features.columns:\n",
    "    df_features['Age_Difference'] = df_features['Husband_age'] - df_features[\"Wife's_current_age\"]\n",
    "\n",
    "feature_cols = [c for c in df_features.columns if c not in target_cols]\n",
    "X = df_features[feature_cols].copy()\n",
    "y = df_features[target_cols].copy()\n",
    "\n",
    "# --- Robust Missing Value Handling ---\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "if len(numeric_cols) > 0:\n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "if len(categorical_cols) > 0:\n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Encode Categorical Features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Encode Targets\n",
    "label_encoders = {}\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y[col] = le.fit_transform(y[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- The Strategic XGBoost Setup ---\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='mlogloss', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Strategic Chain Order based on your previous accuracy results:\n",
    "# Easiest -> Hardest\n",
    "# Index 3: Insulted (92.8%)\n",
    "# Index 2: Humiliated (91.8%)\n",
    "# Index 4: Pushed/Shook (89.2%)\n",
    "# Index 1: Insists knowing where wife is (80.6%)\n",
    "# Index 5: Slapped (76.5%)\n",
    "# Index 0: Jealous (74.9%)\n",
    "# Index 6: Health care decider (74.1%)\n",
    "# Index 7: Household purchases decider (74.1%)\n",
    "# Index 8: Wife afraid (66.2%)\n",
    "optimal_order = [3, 2, 4, 1, 5, 0, 6, 7, 8]\n",
    "\n",
    "# Apply the custom order to the ClassifierChain\n",
    "chain_xgb = ClassifierChain(xgb, order=optimal_order, random_state=42)\n",
    "\n",
    "print(\"Training Strategically Ordered ClassifierChain XGBoost model...\")\n",
    "chain_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred_test = chain_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Test Accuracy per Survey Question ---\")\n",
    "test_accuracies = []\n",
    "for i, col in enumerate(target_cols):\n",
    "    acc = accuracy_score(y_test.iloc[:, i], y_pred_test[:, i])\n",
    "    test_accuracies.append(acc)\n",
    "    print(f\"{col}: {acc * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\n======================================\")\n",
    "print(f\"Average Overall Test Accuracy: {np.mean(test_accuracies) * 100:.2f}%\")\n",
    "print(f\"======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65713d36",
   "metadata": {},
   "source": [
    "USING THE DATASET BALANCING TECHNIQUES TO IMPROVE MODEL LEARNING:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d3deac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e34c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46488\n",
      "==================================================\n",
      "DATASET IMBALANCE (FREQUENCY PER QUESTION)\n",
      "==================================================\n",
      "\n",
      "Husband_partner_jealous_if_Wife_talks_with_other_men\n",
      "No            34834\n",
      "Yes           11498\n",
      "Don't know      156\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Husband_partner_insists_on_knowing_where_Wife_is\n",
      "No            37603\n",
      "Yes            8821\n",
      "Don't know       64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_humiliated_by_husband_partner\n",
      "Never                                 42919\n",
      "Sometimes                              2540\n",
      "Often                                   592\n",
      "Yes, but not in the last 12 months      437\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_insulted_or_made_to_feel_bad_by_husband_partner\n",
      "Never                                 43320\n",
      "Sometimes                              2258\n",
      "Often                                   523\n",
      "Yes, but not in the last 12 months      387\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner\n",
      "Never                                 41446\n",
      "Sometimes                              3576\n",
      "Yes, but not in the last 12 months      902\n",
      "Often                                   564\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Ever_been_slapped_by_husband_partner\n",
      "Never                                 35675\n",
      "Sometimes                              7566\n",
      "Yes, but not in the last 12 months     2402\n",
      "Often                                   845\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Person_who_usually_decides_on_Wife's_health_care\n",
      "Respondent and husband/partner    34154\n",
      "Husband/partner alone              7898\n",
      "Respondent alone                   3901\n",
      "Someone else                        355\n",
      "Other                               180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Person_who_usually_decides_on_large_household_purchases\n",
      "Respondent and husband/partner    34434\n",
      "Husband/partner alone              7970\n",
      "Respondent alone                   2881\n",
      "Someone else                        903\n",
      "Other                               300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never\n",
      "Sometimes afraid           30613\n",
      "Never afraid               11125\n",
      "Most of the time afraid     4750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"completeresponse_dhs.csv\")\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# --- PRINT FREQUENCIES FOR THE REVIEW BOARD ---\n",
    "print(\"==================================================\")\n",
    "print(\"DATASET IMBALANCE (FREQUENCY PER QUESTION)\")\n",
    "print(\"==================================================\\n\")\n",
    "for col in target_cols:\n",
    "    print(df[col].value_counts())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605e2c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed8e37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING INDEPENDENT BINARY MODELS WITH 1:1 UNDERSAMPLING\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Analyzing: Husband_partner_jealous_if_Wife_talks_with_other_men ---\n",
      "Training Set Distribution: Class 0: 9198 | Class 1: 9198\n",
      "Overall Accuracy: 61.98%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.82      0.63      0.71      6967\n",
      "  High Risk (1)       0.34      0.58      0.43      2300\n",
      "\n",
      "       accuracy                           0.62      9267\n",
      "      macro avg       0.58      0.61      0.57      9267\n",
      "   weighted avg       0.70      0.62      0.64      9267\n",
      "\n",
      "\n",
      "--- Analyzing: Husband_partner_insists_on_knowing_where_Wife_is ---\n",
      "Training Set Distribution: Class 0: 7057 | Class 1: 7057\n",
      "Overall Accuracy: 57.81%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.86      0.57      0.69      7521\n",
      "  High Risk (1)       0.25      0.60      0.35      1764\n",
      "\n",
      "       accuracy                           0.58      9285\n",
      "      macro avg       0.55      0.59      0.52      9285\n",
      "   weighted avg       0.74      0.58      0.62      9285\n",
      "\n",
      "\n",
      "--- Analyzing: Ever_been_humiliated_by_husband_partner ---\n",
      "Training Set Distribution: Class 0: 2855 | Class 1: 2855\n",
      "Overall Accuracy: 60.55%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.94      0.61      0.74      8584\n",
      "  High Risk (1)       0.11      0.56      0.18       714\n",
      "\n",
      "       accuracy                           0.61      9298\n",
      "      macro avg       0.53      0.59      0.46      9298\n",
      "   weighted avg       0.88      0.61      0.70      9298\n",
      "\n",
      "\n",
      "--- Analyzing: Ever_been_insulted_or_made_to_feel_bad_by_husband_partner ---\n",
      "Training Set Distribution: Class 0: 2534 | Class 1: 2534\n",
      "Overall Accuracy: 60.29%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.95      0.60      0.74      8664\n",
      "  High Risk (1)       0.10      0.60      0.17       634\n",
      "\n",
      "       accuracy                           0.60      9298\n",
      "      macro avg       0.53      0.60      0.46      9298\n",
      "   weighted avg       0.90      0.60      0.70      9298\n",
      "\n",
      "\n",
      "--- Analyzing: Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner ---\n",
      "Training Set Distribution: Class 0: 4034 | Class 1: 4034\n",
      "Overall Accuracy: 60.83%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.93      0.61      0.73      8290\n",
      "  High Risk (1)       0.16      0.63      0.26      1008\n",
      "\n",
      "       accuracy                           0.61      9298\n",
      "      macro avg       0.55      0.62      0.50      9298\n",
      "   weighted avg       0.85      0.61      0.68      9298\n",
      "\n",
      "\n",
      "--- Analyzing: Ever_been_slapped_by_husband_partner ---\n",
      "Training Set Distribution: Class 0: 8650 | Class 1: 8650\n",
      "Overall Accuracy: 59.29%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.86      0.56      0.68      7135\n",
      "  High Risk (1)       0.33      0.70      0.45      2163\n",
      "\n",
      "       accuracy                           0.59      9298\n",
      "      macro avg       0.59      0.63      0.56      9298\n",
      "   weighted avg       0.74      0.59      0.62      9298\n",
      "\n",
      "\n",
      "--- Analyzing: Person_who_usually_decides_on_Wife's_health_care ---\n",
      "Training Set Distribution: Class 0: 6318 | Class 1: 6318\n",
      "Overall Accuracy: 58.44%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.87      0.58      0.70      7611\n",
      "  High Risk (1)       0.23      0.59      0.33      1580\n",
      "\n",
      "       accuracy                           0.58      9191\n",
      "      macro avg       0.55      0.59      0.51      9191\n",
      "   weighted avg       0.76      0.58      0.64      9191\n",
      "\n",
      "\n",
      "--- Analyzing: Person_who_usually_decides_on_large_household_purchases ---\n",
      "Training Set Distribution: Class 0: 6376 | Class 1: 6376\n",
      "Overall Accuracy: 54.25%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.86      0.53      0.66      7463\n",
      "  High Risk (1)       0.22      0.61      0.32      1594\n",
      "\n",
      "       accuracy                           0.54      9057\n",
      "      macro avg       0.54      0.57      0.49      9057\n",
      "   weighted avg       0.75      0.54      0.60      9057\n",
      "\n",
      "\n",
      "--- Analyzing: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ---\n",
      "Training Set Distribution: Class 1: 8900 | Class 0: 8900\n",
      "Overall Accuracy: 67.57%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.37      0.50      0.42      2225\n",
      "  High Risk (1)       0.82      0.73      0.77      7073\n",
      "\n",
      "       accuracy                           0.68      9298\n",
      "      macro avg       0.60      0.61      0.60      9298\n",
      "   weighted avg       0.71      0.68      0.69      9298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data (assuming df is your raw dataset)\n",
    "# df = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "# --- TARGET GROUPING / BINARIZATION ---\n",
    "# We map the fragmented strings into a clean 0 (Stable) or 1 (High-Risk)\n",
    "grouping_rules = {\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men': {'No': 0, 'Yes': 1},\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is': {'No': 0, 'Yes': 1},\n",
    "    'Ever_been_humiliated_by_husband_partner': {'Never': 0, 'Sometimes': 1, 'Often': 1, 'Yes, but not in the last 12 months': 1},\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner': {'Never': 0, 'Sometimes': 1, 'Often': 1, 'Yes, but not in the last 12 months': 1},\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner': {'Never': 0, 'Sometimes': 1, 'Often': 1, 'Yes, but not in the last 12 months': 1},\n",
    "    'Ever_been_slapped_by_husband_partner': {'Never': 0, 'Sometimes': 1, 'Often': 1, 'Yes, but not in the last 12 months': 1},\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\": {'Respondent and husband/partner': 0, 'Respondent alone': 0, 'Husband/partner alone': 1},\n",
    "    'Person_who_usually_decides_on_large_household_purchases': {'Respondent and husband/partner': 0, 'Respondent alone': 0, 'Husband/partner alone': 1},\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never': {'Never afraid': 0, 'Sometimes afraid': 1, 'Most of the time afraid': 1}\n",
    "}\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"TRAINING INDEPENDENT BINARY MODELS WITH 1:1 UNDERSAMPLING\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# Loop through each question independently, as the majority/minority indices shift per question\n",
    "for col in target_cols:\n",
    "    print(f\"\\n--- Analyzing: {col} ---\")\n",
    "    \n",
    "    # Extract current target and features\n",
    "    df_current = df.copy()\n",
    "    \n",
    "    # Apply the grouping rules. Anything not in the dictionary (like \"Don't know\" or \"Someone else\") becomes NaN\n",
    "    df_current[col] = df_current[col].map(grouping_rules[col])\n",
    "    \n",
    "    # Drop rows where the target is NaN (cleaning the \"Don't knows\")\n",
    "    df_current = df_current.dropna(subset=[col])\n",
    "    \n",
    "    X = df_current.drop(columns=target_cols)\n",
    "    y = df_current[col].astype(int)\n",
    "    \n",
    "    # Handle Features (Missing Values & Encoding)\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "    if len(numeric_cols) > 0: X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "    if len(categorical_cols) > 0: X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "    \n",
    "    X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "    \n",
    "    # 1. Train-Test Split (80/20) BEFORE balancing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # 2. Extract Minority and Majority from the TRAINING set dynamically\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # Let pandas figure out which is the bigger class\n",
    "    class_counts = train_data[col].value_counts()\n",
    "    majority_label = class_counts.idxmax()\n",
    "    minority_label = class_counts.idxmin()\n",
    "    \n",
    "    majority_class = train_data[train_data[col] == majority_label]\n",
    "    minority_class = train_data[train_data[col] == minority_label]\n",
    "    \n",
    "    # 3. Downsample Majority Class to match Minority length\n",
    "    majority_downsampled = resample(majority_class, \n",
    "                                    replace=False,    # sample without replacement\n",
    "                                    n_samples=len(minority_class), # exactly match minority length\n",
    "                                    random_state=42)\n",
    "    \n",
    "    # 4. Combine into a perfectly balanced 1:1 training set\n",
    "    train_balanced = pd.concat([majority_downsampled, minority_class])\n",
    "    \n",
    "    X_train_bal = train_balanced.drop(columns=[col])\n",
    "    y_train_bal = train_balanced[col]\n",
    "    \n",
    "    print(f\"Training Set Distribution: Class {majority_label}: {len(majority_downsampled)} | Class {minority_label}: {len(minority_class)}\")\n",
    "    \n",
    "    # 5. Train XGBoost on the Balanced Data\n",
    "    xgb = XGBClassifier(learning_rate=0.05, n_estimators=150, max_depth=5, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "    xgb.fit(X_train_bal, y_train_bal)\n",
    "    \n",
    "    # 6. Evaluate on the untouched, imbalanced Test Set\n",
    "    y_pred = xgb.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Overall Accuracy: {acc * 100:.2f}%\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Safe/Stable (0)', 'High Risk (1)'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abb8282",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d250d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"completeresponse_dhs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bb8567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding targets and running K-Means Clustering...\n",
      "Dataset Binarized: 0 (Stable) = 35844 cases | 1 (Unstable/Risk) = 10644 cases\n",
      "\n",
      "Oversampling minority class in the TRAINING set using Random Oversampling...\n",
      "Balanced Training Set Distribution: 0: 28675 | 1: 28675\n",
      "\n",
      "Training XGBoost on Balanced Data...\n",
      "\n",
      "==================================================\n",
      "=== FINAL MODEL PERFORMANCE ON UNTOUCHED TEST SET ===\n",
      "==================================================\n",
      "Overall Accuracy: 59.58%\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.82      0.61      0.70      7169\n",
      "  High Risk (1)       0.30      0.56      0.39      2129\n",
      "\n",
      "       accuracy                           0.60      9298\n",
      "      macro avg       0.56      0.58      0.54      9298\n",
      "   weighted avg       0.70      0.60      0.63      9298\n",
      "\n",
      "\n",
      "Saving Models to Disk...\n",
      "Success! Models saved as .pkl files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib # Library for saving models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler # <-- CHANGED TO RANDOM OVERSAMPLING\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data (assuming df is your raw dataset)\n",
    "# df = pd.read_csv('your_actual_file.csv')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "# Drop rows missing targets\n",
    "df_clean = df.dropna(subset=target_cols).copy()\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: TARGET ENCODING & CLUSTERING\n",
    "# ==========================================\n",
    "print(\"Encoding targets and running K-Means Clustering...\")\n",
    "y_raw = df_clean[target_cols].copy()\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode the 9 targets and save the encoders\n",
    "for col in target_cols:\n",
    "    le = LabelEncoder()\n",
    "    y_raw[col] = le.fit_transform(y_raw[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Cluster into 2 groups (Stable vs Unstable)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df_clean['Marital_Stability_Cluster'] = kmeans.fit_predict(y_raw)\n",
    "single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "# Dynamically map the minority cluster to '1' (High Risk) and majority to '0' (Stable)\n",
    "class_counts = df_clean[single_target].value_counts()\n",
    "majority_label = class_counts.idxmax()\n",
    "minority_label = class_counts.idxmin()\n",
    "\n",
    "# Remap to ensure XGBoost gets strict 0 and 1\n",
    "df_clean[single_target] = df_clean[single_target].map({majority_label: 0, minority_label: 1})\n",
    "\n",
    "print(f\"Dataset Binarized: 0 (Stable) = {class_counts.max()} cases | 1 (Unstable/Risk) = {class_counts.min()} cases\")\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 2: FEATURE PREPARATION & SPLITTING\n",
    "# ==========================================\n",
    "feature_cols = [c for c in df_clean.columns if c not in target_cols and c != single_target]\n",
    "X = df_clean[feature_cols].copy()\n",
    "y = df_clean[single_target].copy()\n",
    "\n",
    "# Handle Missing Values\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "if len(numeric_cols) > 0: X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "if len(categorical_cols) > 0: X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Dummy Encode Categorical Features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# SPLIT FIRST to prevent Data Leakage!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 3: RANDOM OVERSAMPLING\n",
    "# ==========================================\n",
    "print(\"\\nOversampling minority class in the TRAINING set using Random Oversampling...\")\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Balanced Training Set Distribution: 0: {sum(y_train_bal==0)} | 1: {sum(y_train_bal==1)}\")\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 4: MODEL TRAINING & EVALUATION\n",
    "# ==========================================\n",
    "print(\"\\nTraining XGBoost on Balanced Data...\")\n",
    "xgb = XGBClassifier(learning_rate=0.05, n_estimators=150, max_depth=5, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "xgb.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"=== FINAL MODEL PERFORMANCE ON UNTOUCHED TEST SET ===\")\n",
    "print(\"==================================================\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Safe/Stable (0)', 'High Risk (1)']))\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 5: SAVING THE MODELS\n",
    "# ==========================================\n",
    "print(\"\\nSaving Models to Disk...\")\n",
    "\n",
    "# Save K-Means Model\n",
    "joblib.dump(kmeans, 'kmeans_cluster_model.pkl')\n",
    "# Save Target Label Encoders\n",
    "joblib.dump(label_encoders, 'target_label_encoders.pkl')\n",
    "# Save the XGBoost Model\n",
    "joblib.dump(xgb, 'xgboost_binary_model.pkl')\n",
    "\n",
    "print(\"Success! Models saved as .pkl files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee000f4",
   "metadata": {},
   "source": [
    "Cluster profiling: Proof that K-Means cluster grouping the stable and unstable class based on \n",
    "positive and negative ansers in the 9 survey questions\n",
    "\n",
    "Phase 1: Overcoming Algorithmic Bias in Target Creation\n",
    "The Unsupervised Learning Trap (K-Means Clustering)\n",
    "Initially, an unsupervised K-Means clustering algorithm was deployed to dynamically segment the dataset into two underlying classes based on the 9 marital survey questions. However, subsequent Cluster Profiling (Centroid Analysis) using cross-tabulation exposed a critical flaw: the algorithm fell into a \"Variance Trap.\"\n",
    "\n",
    "Because instances of extreme physical abuse are statistically rarer than instances of financial inequality, the K-Means algorithm optimized for the wrong dimension. It grouped women who had never been slapped together with women who had been slapped \"Often,\" simply because they shared similar household purchasing dynamics. Unsupervised math failed to distinguish between lethal red flags and mundane relationship friction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f07c62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CLUSTER PROFILING: MATHEMATICAL PROOF OF LABELS\n",
      "==================================================\n",
      "\n",
      "--- Distribution for: Ever_been_slapped_by_husband_partner ---\n",
      "Ever_been_slapped_by_husband_partner  Never  Often  Sometimes  Yes, but not in the last 12 months\n",
      "Marital_Stability_Cluster                                                                        \n",
      "0                                      78.8    1.4       15.0                                 4.8\n",
      "1                                      69.9    3.2       20.6                                 6.3\n",
      "\n",
      "\n",
      "--- Distribution for: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ---\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never  Most of the time afraid  Never afraid  Sometimes afraid\n",
      "Marital_Stability_Cluster                                                                                                   \n",
      "0                                                                                        8.6          25.3              66.1\n",
      "1                                                                                       15.5          19.4              65.1\n",
      "\n",
      "\n",
      "--- Distribution for: Person_who_usually_decides_on_Wife's_health_care ---\n",
      "Person_who_usually_decides_on_Wife's_health_care  Husband/partner alone  Other  Respondent alone  Respondent and husband/partner  Someone else\n",
      "Marital_Stability_Cluster                                                                                                                     \n",
      "0                                                                   0.4    0.1               9.0                            89.6           1.0\n",
      "1                                                                  73.0    1.4               6.4                            19.2           0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_clean already has the 'Marital_Stability_Cluster' column attached from your K-Means step\n",
    "# single_target = 'Marital_Stability_Cluster'\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"CLUSTER PROFILING: MATHEMATICAL PROOF OF LABELS\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# Let's look at a few highly critical questions to prove the difference\n",
    "critical_questions = [\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\"\n",
    "]\n",
    "\n",
    "for col in critical_questions:\n",
    "    print(f\"--- Distribution for: {col} ---\")\n",
    "    \n",
    "    # Create a cross-tabulation showing the percentage of each answer within the two clusters\n",
    "    crosstab = pd.crosstab(\n",
    "        index=df_clean['Marital_Stability_Cluster'], \n",
    "        columns=df_clean[col], \n",
    "        normalize='index' # Converts counts to percentages per cluster\n",
    "    ) * 100\n",
    "    \n",
    "    # Format for clean printing\n",
    "    print(crosstab.round(1).to_string())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a892e9",
   "metadata": {},
   "source": [
    "The Shift to Domain-Driven Determinism\n",
    "To mathematically guarantee that the target variable accurately reflected marital danger, the methodology was pivoted to a Deterministic Rule-Based Composite Target.\n",
    "\n",
    "Drawing directly from the sociological severity of the survey, a strict operational definition was established:\n",
    "\n",
    "Class 1 (High-Risk/Unstable): Any demographic profile where the respondent reported any instance of physical violence, extreme fear, or complete loss of healthcare/financial agency (e.g., answering \"Often,\" \"Sometimes,\" or \"Yes\").\n",
    "\n",
    "Class 0 (Stable): A strict baseline requiring absolute zero red flags across all 9 survey dimensions.\n",
    "\n",
    "By explicitly prioritizing the severe abuse indicators, the target variable was successfully tethered to the reality of the dataset rather than blind algorithmic variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07899c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marital_Stability_Target\n",
      "1    25987\n",
      "0    20501\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the Red Flag answers for ALL 9 questions\n",
    "red_flag_answers = [\n",
    "    'Yes', \n",
    "    'Often', \n",
    "    'Sometimes', \n",
    "    'Most of the time afraid',\n",
    "    'Husband/partner alone',\n",
    "]\n",
    "\n",
    "# 2. The function that reads a woman's entire survey profile at once\n",
    "def determine_risk(row):\n",
    "    # We put ALL 9 target columns here\n",
    "    all_9_targets = [\n",
    "        'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "        'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "        'Ever_been_humiliated_by_husband_partner',\n",
    "        'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "        'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "        'Ever_been_slapped_by_husband_partner',\n",
    "        \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "        'Person_who_usually_decides_on_large_household_purchases',\n",
    "        'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "    ]\n",
    "    \n",
    "    # Check every single question for this specific woman\n",
    "    for col in all_9_targets:\n",
    "        if row[col] in red_flag_answers:\n",
    "            return 1 # If she answered ANY red flag, mark her marriage as High Risk (1)\n",
    "            \n",
    "    return 0 # If she has ZERO red flags across all 9 questions, mark as Stable (0)\n",
    "\n",
    "# 3. Apply it to the entire dataframe at once!\n",
    "# axis=1 means \"read across the row\"\n",
    "df_clean['Marital_Stability_Target'] = df_clean.apply(determine_risk, axis=1)\n",
    "\n",
    "# Print the final, single binary distribution\n",
    "print(df_clean['Marital_Stability_Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0a1bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "POST-HOC PROFILING: PROOF OF DETERMINISTIC TARGET\n",
      "==================================================\n",
      "\n",
      "--- Distribution for: Ever_been_slapped_by_husband_partner ---\n",
      "Ever_been_slapped_by_husband_partner  Never  Often  Sometimes  Yes, but not in the last 12 months\n",
      "Marital_Stability_Target                                                                         \n",
      "0                                      95.9    0.0        0.0                                 4.1\n",
      "1                                      61.6    3.3       29.1                                 6.0\n",
      "\n",
      "\n",
      "--- Distribution for: Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner ---\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner  Never  Often  Sometimes  Yes, but not in the last 12 months\n",
      "Marital_Stability_Target                                                                                                       \n",
      "0                                                                    98.9    0.0        0.0                                 1.1\n",
      "1                                                                    81.4    2.2       13.8                                 2.6\n",
      "\n",
      "\n",
      "--- Distribution for: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ---\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never  Most of the time afraid  Never afraid  Sometimes afraid\n",
      "Marital_Stability_Target                                                                                                    \n",
      "0                                                                                        0.0          33.3              66.7\n",
      "1                                                                                       18.3          16.5              65.2\n",
      "\n",
      "\n",
      "--- Distribution for: Person_who_usually_decides_on_Wife's_health_care ---\n",
      "Person_who_usually_decides_on_Wife's_health_care  Husband/partner alone  Other  Respondent alone  Respondent and husband/partner  Someone else\n",
      "Marital_Stability_Target                                                                                                                      \n",
      "0                                                                   0.0    0.4               8.0                            90.7           0.9\n",
      "1                                                                  30.4    0.3               8.7                            59.9           0.7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_clean already has the newly created 'Marital_Stability_Target'\n",
    "print(\"==================================================\")\n",
    "print(\"POST-HOC PROFILING: PROOF OF DETERMINISTIC TARGET\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "critical_questions = [\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\"\n",
    "]\n",
    "\n",
    "for col in critical_questions:\n",
    "    print(f\"--- Distribution for: {col} ---\")\n",
    "    \n",
    "    # Create the cross-tabulation table\n",
    "    crosstab = pd.crosstab(\n",
    "        index=df_clean['Marital_Stability_Target'], \n",
    "        columns=df_clean[col], \n",
    "        normalize='index' # Converts to percentage per cluster\n",
    "    ) * 100\n",
    "    \n",
    "    print(crosstab.round(1).to_string())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d4de08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Set Distribution:\n",
      "Marital_Stability_Target\n",
      "1    20789\n",
      "0    16401\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balancing the training set using Random Oversampling...\n",
      "\n",
      "Balanced Training Set Distribution:\n",
      "Marital_Stability_Target\n",
      "1    20789\n",
      "0    20789\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training XGBoost Classifier...\n",
      "\n",
      "==================================================\n",
      "=== FINAL DETERMINISTIC MODEL PERFORMANCE ===\n",
      "==================================================\n",
      "Overall Accuracy: 69.35%\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Safe/Stable (0)       0.61      0.85      0.71      4100\n",
      "  High Risk (1)       0.83      0.57      0.68      5198\n",
      "\n",
      "       accuracy                           0.69      9298\n",
      "      macro avg       0.72      0.71      0.69      9298\n",
      "   weighted avg       0.73      0.69      0.69      9298\n",
      "\n",
      "\n",
      "Model and feature columns successfully saved to disk as .pkl files.\n",
      "\n",
      "==================================================\n",
      "=== GENERATING DECODED BALANCED DATASET ===\n",
      "==================================================\n",
      "Success! Decoded training data saved as 'Decoded_Balanced_Training_Data.csv'.\n",
      "You can now download this file from your Codespace Explorer for the T-Test and Chi-Square audit.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib \n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Prepare Features (X) and Target (y)\n",
    "# (Assuming df_clean already contains the 9 target columns and the new 'Marital_Stability_Target')\n",
    "\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "# Drop the 9 original target questions, keeping only demographics + the new composite target\n",
    "X = df_clean.drop(columns=target_cols + ['Marital_Stability_Target'])\n",
    "y = df_clean['Marital_Stability_Target']\n",
    "\n",
    "# 2. Handle Missing Values and Encode Categorical Demographics\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "if len(numeric_cols) > 0: \n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "if len(categorical_cols) > 0: \n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Dummy Encode Categorical Features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# 3. SPLIT FIRST to prevent Data Leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Original Training Set Distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# 4. RANDOM OVERSAMPLING (Only on Training Data)\n",
    "print(\"\\nBalancing the training set using Random Oversampling...\")\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nBalanced Training Set Distribution:\")\n",
    "print(y_train_bal.value_counts())\n",
    "\n",
    "# 5. TRAIN XGBOOST\n",
    "print(\"\\nTraining XGBoost Classifier...\")\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.03, \n",
    "    n_estimators=600, \n",
    "    max_depth=6, \n",
    "    eval_metric='logloss', \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# 6. EVALUATE ON UNTOUCHED TEST SET\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"=== FINAL DETERMINISTIC MODEL PERFORMANCE ===\")\n",
    "print(\"==================================================\")\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Safe/Stable (0)', 'High Risk (1)']))\n",
    "\n",
    "# 7. SAVE THE DEPLOYMENT MODEL\n",
    "joblib.dump(xgb, 'final_xgboost_deterministic_model.pkl')\n",
    "# Save the exact feature columns so your deployment tool knows the required input structure\n",
    "joblib.dump(X_encoded.columns.tolist(), 'model_feature_columns.pkl')\n",
    "\n",
    "print(\"\\nModel and feature columns successfully saved to disk as .pkl files.\")\n",
    "\n",
    "# ==================================================\n",
    "# 8. GENERATE AND SAVE DECODED BALANCED DATASET\n",
    "# ==================================================\n",
    "print(\"\\n==================================================\")\n",
    "print(\"=== GENERATING DECODED BALANCED DATASET ===\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# X is our imputed but UNENCODED feature set. \n",
    "# We apply the exact same split seed to get the exact same rows.\n",
    "X_train_txt, X_test_txt, y_train_txt, y_test_txt = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply the exact same RandomOverSampler seed\n",
    "ros_txt = RandomOverSampler(random_state=42)\n",
    "X_train_bal_txt, y_train_bal_txt = ros_txt.fit_resample(X_train_txt, y_train_txt)\n",
    "\n",
    "# Combine the unencoded features and the target into one clean DataFrame\n",
    "decoded_export_df = X_train_bal_txt.copy()\n",
    "decoded_export_df['Marital_Stability_Target'] = y_train_bal_txt\n",
    "\n",
    "# Save to CSV\n",
    "csv_filename = 'Decoded_Balanced_Training_Data.csv'\n",
    "decoded_export_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Success! Decoded training data saved as '{csv_filename}'.\")\n",
    "print(\"You can now download this file from your Codespace Explorer for the T-Test and Chi-Square audit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84f3519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GENERATING DECODED BALANCED DATASET\n",
      "==================================================\n",
      "\n",
      "Success! 'Balanced_Training_Data.csv' saved to your folder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"GENERATING DECODED BALANCED DATASET\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# Assuming 'X' is your features dataframe right BEFORE you applied pd.get_dummies()\n",
    "# and 'y' is your Marital_Stability_Target\n",
    "X_unencoded = X.copy()\n",
    "\n",
    "# 1. Split the unencoded text data using the exact same seed (42)\n",
    "X_train_txt, X_test_txt, y_train_txt, y_test_txt = train_test_split(\n",
    "    X_unencoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Oversample the unencoded text data using the exact same seed (42)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_bal_txt, y_train_bal_txt = ros.fit_resample(X_train_txt, y_train_txt)\n",
    "\n",
    "# 3. Combine and Save to CSV\n",
    "decoded_export_df = X_train_bal_txt.copy()\n",
    "decoded_export_df['Marital_Stability_Target'] = y_train_bal_txt\n",
    "\n",
    "decoded_export_df.to_csv('Balanced_Training_Data.csv', index=False)\n",
    "print(\"Success! 'Balanced_Training_Data.csv' saved to your folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337b5efc",
   "metadata": {},
   "source": [
    "Only Rule based target derivation without clustering ---> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e181477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PHASE 1: DATA LOADING & TARGET DERIVATION\n",
      "==================================================\n",
      "\n",
      "Target Derivation Complete. Class Distribution:\n",
      "Marital_Stability_Target\n",
      "1    39877\n",
      "0     6611\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "PHASE 2: FEATURE PREPARATION & SPLITTING\n",
      "==================================================\n",
      "\n",
      "Training Set Dimensions before balancing: (37190, 84)\n",
      "\n",
      "==================================================\n",
      "PHASE 3: RANDOM OVERSAMPLING & MODEL TRAINING\n",
      "==================================================\n",
      "\n",
      "Balancing the minority class in the training set...\n",
      "Balanced Training Class Distribution:\n",
      "Marital_Stability_Target\n",
      "0    31901\n",
      "1    31901\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training XGBoost Classifier...\n",
      "\n",
      "==================================================\n",
      "PHASE 4: FINAL EVALUATION ON UNTOUCHED TEST SET\n",
      "==================================================\n",
      "\n",
      "Overall Accuracy: 68.26%\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "       Stable / Safe (0)       0.24      0.56      0.33      1322\n",
      "High Risk / Unstable (1)       0.91      0.70      0.79      7976\n",
      "\n",
      "                accuracy                           0.68      9298\n",
      "               macro avg       0.57      0.63      0.56      9298\n",
      "            weighted avg       0.81      0.68      0.73      9298\n",
      "\n",
      "Success! Final model and feature schema saved to disk.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib \n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"PHASE 1: DATA LOADING & TARGET DERIVATION\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# 1. Load the raw data\n",
    "# Replace with your actual file path if different\n",
    "df = pd.read_csv('completeresponse_dhs.csv')\n",
    "\n",
    "# The 9 original survey questions\n",
    "target_cols = [\n",
    "    'Husband_partner_jealous_if_Wife_talks_with_other_men',\n",
    "    'Husband_partner_insists_on_knowing_where_Wife_is',\n",
    "    'Ever_been_humiliated_by_husband_partner',\n",
    "    'Ever_been_insulted_or_made_to_feel_bad_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\",\n",
    "    'Person_who_usually_decides_on_large_household_purchases',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never'\n",
    "]\n",
    "\n",
    "# Drop rows where any of the critical survey answers are missing to ensure clean rule processing\n",
    "df_clean = df.dropna(subset=target_cols).copy()\n",
    "\n",
    "# 2. Define the exact red-flag strings (Updated to catch all variations)\n",
    "red_flag_answers = [\n",
    "    'Yes', \n",
    "    'Often', \n",
    "    'Sometimes', \n",
    "    'Yes, but not in the last 12 months', \n",
    "    'Most of the time afraid',\n",
    "    'Sometimes afraid',                   \n",
    "    'Husband/partner alone' \n",
    "]\n",
    "\n",
    "# 3. Create the Deterministic Rule Function\n",
    "def determine_risk(row):\n",
    "    for col in target_cols:\n",
    "        if row[col] in red_flag_answers:\n",
    "            return 1 # High Risk / Unstable (At least one red flag)\n",
    "    return 0 # Stable (Zero red flags)\n",
    "\n",
    "# Apply the rule to create the single binary target\n",
    "df_clean['Marital_Stability_Target'] = df_clean.apply(determine_risk, axis=1)\n",
    "\n",
    "print(\"Target Derivation Complete. Class Distribution:\")\n",
    "print(df_clean['Marital_Stability_Target'].value_counts())\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"PHASE 2: FEATURE PREPARATION & SPLITTING\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# Drop the original 9 survey questions; keep demographics and the new target\n",
    "X = df_clean.drop(columns=target_cols + ['Marital_Stability_Target'])\n",
    "y = df_clean['Marital_Stability_Target']\n",
    "\n",
    "# Identify column types\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'str']).columns\n",
    "\n",
    "# Impute missing values\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "\n",
    "if len(numeric_cols) > 0: \n",
    "    X[numeric_cols] = num_imputer.fit_transform(X[numeric_cols])\n",
    "if len(categorical_cols) > 0: \n",
    "    X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Dummy encode categorical features\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# STRATIFIED SPLIT FIRST to prevent Data Leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training Set Dimensions before balancing: {X_train.shape}\")\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"PHASE 3: RANDOM OVERSAMPLING & MODEL TRAINING\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "print(\"Balancing the minority class in the training set...\")\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Balanced Training Class Distribution:\")\n",
    "print(y_train_bal.value_counts())\n",
    "\n",
    "print(\"\\nTraining XGBoost Classifier...\")\n",
    "xgb = XGBClassifier(\n",
    "    learning_rate=0.03, \n",
    "    n_estimators=600, \n",
    "    max_depth=6, \n",
    "    eval_metric='logloss', \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "print(\"PHASE 4: FINAL EVALUATION ON UNTOUCHED TEST SET\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Stable / Safe (0)', 'High Risk / Unstable (1)']))\n",
    "\n",
    "# Save the final artifacts for deployment\n",
    "joblib.dump(xgb, 'final_xgboost_model.pkl')\n",
    "joblib.dump(X_encoded.columns.tolist(), 'model_feature_columns.pkl')\n",
    "\n",
    "print(\"Success! Final model and feature schema saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe135385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "POST-HOC PROFILING: PROOF OF DETERMINISTIC TARGET\n",
      "==================================================\n",
      "\n",
      "--- Distribution for: Ever_been_slapped_by_husband_partner ---\n",
      "Ever_been_slapped_by_husband_partner  Never  Often  Sometimes  Yes, but not in the last 12 months\n",
      "Marital_Stability_Target                                                                         \n",
      "0                                     100.0    0.0        0.0                                 0.0\n",
      "1                                      72.9    2.1       19.0                                 6.0\n",
      "\n",
      "\n",
      "--- Distribution for: Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner ---\n",
      "Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner  Never  Often  Sometimes  Yes, but not in the last 12 months\n",
      "Marital_Stability_Target                                                                                                       \n",
      "0                                                                   100.0    0.0        0.0                                 0.0\n",
      "1                                                                    87.4    1.4        9.0                                 2.3\n",
      "\n",
      "\n",
      "--- Distribution for: Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never ---\n",
      "Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never  Most of the time afraid  Never afraid  Sometimes afraid\n",
      "Marital_Stability_Target                                                                                                    \n",
      "0                                                                                        0.0         100.0               0.0\n",
      "1                                                                                       11.9          11.3              76.8\n",
      "\n",
      "\n",
      "--- Distribution for: Person_who_usually_decides_on_Wife's_health_care ---\n",
      "Person_who_usually_decides_on_Wife's_health_care  Husband/partner alone  Other  Respondent alone  Respondent and husband/partner  Someone else\n",
      "Marital_Stability_Target                                                                                                                      \n",
      "0                                                                   0.0    0.5               9.4                            89.4           0.8\n",
      "1                                                                  19.8    0.4               8.2                            70.8           0.8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_clean already has the newly created 'Marital_Stability_Target'\n",
    "print(\"==================================================\")\n",
    "print(\"POST-HOC PROFILING: PROOF OF DETERMINISTIC TARGET\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "critical_questions = [\n",
    "    'Ever_been_slapped_by_husband_partner',\n",
    "    'Ever_been_pushed,_shook_or_had_something_thrown_by_husband_partner',\n",
    "    'Wife_afraid_of_husband_partner_most_of_the_time,_sometimes_or_never',\n",
    "    \"Person_who_usually_decides_on_Wife's_health_care\"\n",
    "]\n",
    "\n",
    "for col in critical_questions:\n",
    "    print(f\"--- Distribution for: {col} ---\")\n",
    "    \n",
    "    # Create the cross-tabulation table\n",
    "    crosstab = pd.crosstab(\n",
    "        index=df_clean['Marital_Stability_Target'], \n",
    "        columns=df_clean[col], \n",
    "        normalize='index' # Converts to percentage per cluster\n",
    "    ) * 100\n",
    "    \n",
    "    print(crosstab.round(1).to_string())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6183d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING DATASETS FOR GLOBAL STATISTICAL AUDIT\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "GLOBAL STATISTICAL PROOF (T-TEST & CHI-SQUARE)\n",
      "==================================================\n",
      "\n",
      "Running T-Tests on Continuous Demographics...\n",
      "Running Chi-Square Tests on Categorical Demographics...\n",
      "\n",
      "--- Global Statistical Hypothesis Test Results ---\n",
      "                     Feature  Test Type  p-value              Conclusion\n",
      "                 Husband_age     T-Test   0.5927 Distributions Identical\n",
      "          Wife's_current_age     T-Test   0.4224 Distributions Identical\n",
      "  Wife's height(centimeters)     T-Test   0.3073 Distributions Identical\n",
      "   Husband's Education Level Chi-Square   0.5008 Distributions Identical\n",
      "Husband Occupation_(grouped) Chi-Square   0.7545 Distributions Identical\n",
      "           Husband Ethnicity Chi-Square   0.7599 Distributions Identical\n",
      "            Husband Religion Chi-Square   0.9896 Distributions Identical\n",
      "      Wife's Education Level Chi-Square   0.3875 Distributions Identical\n",
      " Wife's_occupation_(grouped) Chi-Square   0.9935 Distributions Identical\n",
      "            Wife's Ethnicity Chi-Square   0.6757 Distributions Identical\n",
      "             Wife's Religion Chi-Square   0.9627 Distributions Identical\n",
      "                   residence Chi-Square   0.2971 Distributions Identical\n",
      "                       State Chi-Square   0.8793 Distributions Identical\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"LOADING DATASETS FOR GLOBAL STATISTICAL AUDIT\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "# 1. Load the original raw dataset\n",
    "df_orig = pd.read_csv('completeresponse_dhs.csv')\n",
    "\n",
    "# 2. Load the DECODED balanced training dataset\n",
    "df_bal = pd.read_csv('Decoded_Balanced_Training_Data.csv')\n",
    "\n",
    "print(\"==================================================\")\n",
    "print(\"GLOBAL STATISTICAL PROOF (T-TEST & CHI-SQUARE)\")\n",
    "print(\"==================================================\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- 1. INDEPENDENT T-TEST FOR CONTINUOUS VARIABLES ---\n",
    "continuous_cols = [\"Husband_age\", \"Wife's_current_age\", \"Wife's height(centimeters)\"]\n",
    "\n",
    "print(\"Running T-Tests on Continuous Demographics...\")\n",
    "for col in continuous_cols:\n",
    "    # Drop NAs to avoid mathematical errors during mean calculations\n",
    "    orig_data = df_orig[col].dropna()\n",
    "    bal_data = df_bal[col].dropna()\n",
    "    \n",
    "    # T-Test checks if the overall mathematical mean shifted globally\n",
    "    stat, p_value = ttest_ind(orig_data, bal_data, equal_var=True)\n",
    "    results.append({'Feature': col, 'Test Type': 'T-Test', 'p-value': p_value})\n",
    "\n",
    "\n",
    "# --- 2. CHI-SQUARE TEST FOR CATEGORICAL VARIABLES ---\n",
    "categorical_cols = [\n",
    "    \"Husband's Education Level\", \"Husband Occupation_(grouped)\", \"Husband Ethnicity\", \n",
    "    \"Husband Religion\", \"Wife's Education Level\", \"Wife's_occupation_(grouped)\", \n",
    "    \"Wife's Ethnicity\", \"Wife's Religion\", \"residence\", \"State\"\n",
    "]\n",
    "\n",
    "print(\"Running Chi-Square Tests on Categorical Demographics...\")\n",
    "for col in categorical_cols:\n",
    "    # Directly count the global text categories in both entire datasets\n",
    "    orig_counts = df_orig[col].value_counts()\n",
    "    bal_counts = df_bal[col].value_counts()\n",
    "    \n",
    "    # Align the counts into a single dataframe (contingency table)\n",
    "    contingency_df = pd.DataFrame({'Original': orig_counts, 'Balanced': bal_counts}).fillna(0)\n",
    "    \n",
    "    # Run the Chi-Square test on the global text distributions\n",
    "    stat, p_value, dof, expected = chi2_contingency(contingency_df.T)\n",
    "    results.append({'Feature': col, 'Test Type': 'Chi-Square', 'p-value': p_value})\n",
    "\n",
    "# --- DISPLAY THE FINAL PROOF ---\n",
    "proof_df = pd.DataFrame(results)\n",
    "proof_df['p-value'] = proof_df['p-value'].round(4)\n",
    "\n",
    "# p-value > 0.05 proves the global distributions are statistically identical\n",
    "proof_df['Conclusion'] = np.where(proof_df['p-value'] > 0.05, 'Distributions Identical', 'Distributions Altered')\n",
    "\n",
    "print(\"\\n--- Global Statistical Hypothesis Test Results ---\")\n",
    "print(proof_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
